{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c95846a1-5086-4318-849a-4818da9a8382",
   "metadata": {},
   "source": [
    "## Register for a Hugging Face Account\r\n",
    "If you do not already have a Hugging Face account, you will need to create one. Register at the following link to get started:\r\n",
    "\r\n",
    "- [Register here](https://huggingface.co/) to create a Hugging Face account.\r\n",
    "\r\n",
    "## Use Facebook's Latest Language Model: Meta-Llama-3-8B\r\n",
    "In this notebook, we will be working with Facebook's latest language model, \"meta-llama/Meta-Llama-3-8B\". Make sure to obtain the necessary , refer to [this link](https://huggingface.co/meta-llama/Meta-Llama-3-8B).\n",
    "\n",
    "link.\r\n",
    "\r\n",
    "## Educational Resources\r\n",
    "To maximize the potential of Hugging Face's offerings, familiarize yourself with the following tutorials:\r\n",
    "\r\n",
    "- **Language Model Tutorial:** Learn how to leverage large language models effectively by consulting the [Transformers documentation](https://huggingface.co/docs/transformers/index).\r\n",
    "- **Open Source Dataset Tutorial:** Explore and utilize datasets available on Hugging Face with the [Datasets documentation](https://huggingface.co/docs/datasets/index).\r\n",
    "- **Fine-Tuning Models Tutorial:** For hands-on guidance on fine-tuning models within notebooks, refer to [this tutorial](https://huggingface.co/docs/transformers/notebooks).\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0688a7eb-2d6c-43d7-bbd4-08a359bd66b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b97ac28fad8405ca4ec176f8d49f01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the 'login' function from the huggingface_hub library to authenticate and access your Hugging Face account.\n",
    "from huggingface_hub import login\n",
    "# Call the login function to authenticate. You'll need to enter your credentials or token.\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06afb0b-a31a-460b-b03c-81f31b2e8f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 20:29:18.285930: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-02 20:29:18.636457: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-02 20:29:18.636491: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-02 20:29:18.636522: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-02 20:29:18.652420: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-02 20:29:26.259003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.2+cu118\n",
      "Datasets version: 2.19.1\n",
      "TRL (Transformers Reinforcement Learning) version: 0.8.6\n",
      "PEFT (Parameter Efficient Fine-Tuning) version: 0.11.1\n",
      "Transformers version: 4.40.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from datetime import datetime\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import trl\n",
    "from trl import setup_chat_format,SFTTrainer\n",
    "\n",
    "import peft\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from random import randint\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Datasets version:\", datasets.__version__)\n",
    "print(\"TRL (Transformers Reinforcement Learning) version:\", trl.__version__)\n",
    "print(\"PEFT (Parameter Efficient Fine-Tuning) version:\", peft.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4d2074-f01d-414b-a003-eab664228890",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Large Language Model (LLM) for SQL Query Generation\n",
    "\n",
    "## Notebook Overview\n",
    "This notebook focuses on fine-tuning a Large Language Model (LLM) to perform a specialized task in natural language processing. The objective is to train the model to generate SQL queries based on provided language descriptions (questions) and SQL CREATE TABLE statements (context). \n",
    "\n",
    "## Dataset\n",
    "We will utilize an open-source dataset available on Hugging Face, which is specifically structured for training models to understand and generate SQL queries from natural language descriptions and table contexts.\n",
    "\n",
    "- **Dataset Source:** You can access the dataset and view its detailed structure and samples by visiting [b-mc2/sql-create-context on Hugging Face](https://huggingface.co/datasets/b-mc2/sql-create-context). This dataset provides a rich collection of examples where each entry includes:\n",
    "  - `question`: A natural language description of the data retrieval or manipulation task.\n",
    "  - `context`: SQL CREATE TABLE statements providing the schema of the database relevant to the question.\n",
    "  - `answer`: The SQL query that correctly retrieves or manipulates the data as described in the question.\n",
    "\n",
    "## Objective\n",
    "The goal of this notebook is to demonstrate how to set up, train, and evaluate a model using this dataset.\n",
    "\n",
    "\n",
    "<span style=\"color: red;\">Furthermore, to give you an idea of the data needed for the model, consider what type of data is necessary when training a LLM to generate playwright code for UI testing.</span>\n",
    "\n",
    "\n",
    "\n",
    "1. What is UI Testing?\n",
    "2. What is Playwright and How to Use It?\n",
    "3. What is an LLM?\n",
    "4. How to Use HuggingFace ？\n",
    "5. How to Finetune a Large Model？\n",
    "6. Type of Data Needed for Finetuning for a UI Testing Task？ refer to attached slides.\n",
    "7. Evaluating and Testing the Model's Performance ?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b8c94f-38ce-456a-b4b3-c7d926be0916",
   "metadata": {},
   "source": [
    "Here's some inspiration\n",
    "\n",
    "## Reflection on UI Testing for Software Development\r\n",
    "When integrating this LLM  into a software application, conducting UI tests is crucial to ensure the system functions correctly from the user's perspective. This section details the critical considerations for inputting, processing, and outputting data during UI tests.\r\n",
    "\r\n",
    "### Model Inputs for UI Testing\r\n",
    "- **User Input**: The actual text input by users, simulating real-world usage where they articulate their needs in terms of software interactions. This input tests the model’s ability to interpret and respond to varied, unstructured human language.\r\n",
    "- **Schema Context**: A consistent input that provides the model with the structure of the software during testing, ensuring that the outputs are applicable and correctly formatted. This schema defines the boundaries and possibilities for the user's requests.\r\n",
    "\r\n",
    "### Required Context for UI Testing\r\n",
    "- **Relevance and Completeness**: The model must fully understand the provided software schema and the user's request relevance to generate accurate responses. This understanding is essential for ensuring that the responses are both technically correct and contextually appropriate.\r\n",
    "- **Interaction History**: By maintaining a record of past interactions, the model can refine its responses based on previous questions and answers. This history helps improve the model’s accuracy and adaptability over time, providing more personalized responses to the user.\r\n",
    "\r\n",
    "### Expected Outputs from the Model\r\n",
    "- **Playwright Code**: The model should output accurate Playwright code that fulfills the user’s request based on the given description and software schema. Playwright is used here as a tool for automating browser tasks based on user requirements expressed in natural language.\r\n",
    "- **Feedback Mechanisms**: If the model is unable to generate valid Playwright code or if the request is too ambiguous, it should offer feedback or request further clarification. This mechanism is crucial for identifying the model's limitations during user testing and for improving user experience by preventing misinterpretations and errors in real-world applications.\r\n",
    "\r\n",
    "### Considerations for Effective UI Testing\r\n",
    "- **Test Scenario Coverage**: Develop comprehensive test scenarios that cover a wide range of potential user interactions to ensure the model can handle diverse and unexpected inputs.\r\n",
    "- **Error Handling**: Evaluate how the system responds to incorrect inputs or non-executable requests. Effective error handling is crucial for maintaining usability and user trust.\r\n",
    "- **Performance Metrics**: Assess how quickly the model processes inputs and generates outputs. Response time is a critical factor in user satisfaction, especially in interactive applications where delays can disrupt the user experience.\r\n",
    "\r\n",
    "Implementing thorough UI testing will help guarantee that the model not only performs well under controlled conditions but also operates effectively and reliably in real-world scenarios, enhancing the overall utility and user-friendliness of the software.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6279239-efea-45e4-b0e8-5ef2d8c182cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a system message template for the conversational model.\n",
    "# This message sets the context by describing the role of the assistant and providing a database schema.\n",
    "system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "{schema}\"\"\"\n",
    "\n",
    "# Function to format individual samples from the dataset into a conversation format for training the model.\n",
    "def create_conversation(sample):\n",
    "    # Creates a dictionary representing a conversation with system, user, and assistant messages.\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},  # System role message with schema\n",
    "            {\"role\": \"user\", \"content\": sample[\"question\"]},  # User role message with the query question\n",
    "            {\"role\": \"assistant\", \"content\": sample[\"answer\"]}  # Assistant role message with the SQL query answer\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Load the dataset from the Hugging Face Hub, specifically using the 'train' split.\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "# Shuffle the dataset and select the first 12,500 entries for processing.\n",
    "dataset = dataset.shuffle().select(range(12500))\n",
    "\n",
    "# Apply the create_conversation function to each sample in the dataset, converting them to the required format.\n",
    "# Remove the original columns from the dataset as they are no longer needed after conversion.\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features, batched=False)\n",
    "# Split the dataset into 10,000 training samples and 2,500 test samples.\n",
    "dataset = dataset.train_test_split(test_size=2500/12500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89dca50a-4dff-4b85-a3b7-a3e8055084c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 2500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The `DatasetDict` object contains two subsets of data: `train` and `test`, which are used for training and evaluating the model, respectively.\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ef91c40-2f67-4d1a-a9ec-5d5ee48075d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System Message</th>\n",
       "      <th>User Question</th>\n",
       "      <th>Assistant Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_97 (player VARCHAR, debut VARCHAR, born VARCHAR)</td>\n",
       "      <td>Who was the player with a debut of age 18 v plymouth , 14 august 2012, and born in Portsmouth?</td>\n",
       "      <td>SELECT player FROM table_name_97 WHERE debut = \"age 18 v plymouth , 14 august 2012\" AND born = \"portsmouth\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_17968282_1 (points VARCHAR, team VARCHAR)</td>\n",
       "      <td>Name the total number of points for newell's old boys</td>\n",
       "      <td>SELECT COUNT(points) FROM table_17968282_1 WHERE team = \"Newell's Old Boys\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_19359427_6 (manner_of_departure VARCHAR, outgoing_manager VARCHAR)</td>\n",
       "      <td>What was Gary Megson's manner of departure?</td>\n",
       "      <td>SELECT manner_of_departure FROM table_19359427_6 WHERE outgoing_manager = \"Gary Megson\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_88 (silver INTEGER, total VARCHAR)</td>\n",
       "      <td>What is the average number of silver medals won among nations that won 9 medals total?</td>\n",
       "      <td>SELECT AVG(silver) FROM table_name_88 WHERE total = 9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_17 (silver VARCHAR, location VARCHAR)</td>\n",
       "      <td>What Silver has the Location of Guangzhou?</td>\n",
       "      <td>SELECT silver FROM table_name_17 WHERE location = \"guangzhou\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_55 (circuit VARCHAR, date VARCHAR)</td>\n",
       "      <td>What Circuit has a Date of 25 july?</td>\n",
       "      <td>SELECT circuit FROM table_name_55 WHERE date = \"25 july\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_21 (opponent VARCHAR, game VARCHAR)</td>\n",
       "      <td>Who is the opponent in game 7?</td>\n",
       "      <td>SELECT opponent FROM table_name_21 WHERE game = 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_35 (original_title VARCHAR, director VARCHAR, film_title_used_in_nomination VARCHAR)</td>\n",
       "      <td>What is Original title, when Director is Veljko Bulajić category:articles with hcards, and when Film title used in nomination is Train Without A Timetable?</td>\n",
       "      <td>SELECT original_title FROM table_name_35 WHERE director = \"veljko bulajić category:articles with hcards\" AND film_title_used_in_nomination = \"train without a timetable\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_23285805_6 (score VARCHAR, record VARCHAR)</td>\n",
       "      <td>What was the score with the team record was 15-25?</td>\n",
       "      <td>SELECT score FROM table_23285805_6 WHERE record = \"15-25\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_93 (tournament VARCHAR)</td>\n",
       "      <td>During the Hamburg Masters Tournament, during which Jiří Novák was absent(A) in 1998, how did he do in 1997?</td>\n",
       "      <td>SELECT 1997 FROM table_name_93 WHERE 1998 = \"a\" AND tournament = \"hamburg masters\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def convert_to_dataframe(data):\n",
    "    # Initialize lists to hold data for each column\n",
    "    system_messages = []\n",
    "    user_questions = []\n",
    "    assistant_answers = []\n",
    "    \n",
    "    # Iterate through each conversation in the data\n",
    "    for conversation in data:\n",
    "        # Append each message to the corresponding list\n",
    "        system_messages.append(conversation[0]['content'])\n",
    "        user_questions.append(conversation[1]['content'])\n",
    "        assistant_answers.append(conversation[2]['content'])\n",
    "    \n",
    "    # Create a DataFrame from the lists\n",
    "    df = pd.DataFrame({\n",
    "        'System Message': system_messages,\n",
    "        'User Question': user_questions,\n",
    "        'Assistant Answer': assistant_answers\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df = convert_to_dataframe(dataset[\"train\"][:10][\"messages\"])\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7edbb1b4-0fd7-43f0-b6ef-7bdc72775d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36fa07e6de0451fb67cdccf2f3c8b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8093b4099db5473fae5aeda567a8b51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1193600"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(\"data/train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"data/test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e0a619-8383-4e95-94f7-6c592d293b59",
   "metadata": {},
   "source": [
    "## Loading a Pre-trained Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0203921f-df76-482c-a4fd-ab0a7c3231b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kit/tm/px6680/miniconda3/envs/huggingface/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240816f607244c00ac39c3e9eba7e76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Define the identifier for the model to be loaded, specifically \"meta-llama/Meta-Llama-3-8B\".\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\" \n",
    "\n",
    "# Configure the model for low-precision (4-bit) quantization to reduce memory usage and potentially increase inference speed.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enable loading the model in 4-bit precision\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization for better precision handling\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Set the quantization type to 'nf4', a specific 4-bit format\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Use bfloat16 as the datatype for computation to balance performance and accuracy\n",
    ")\n",
    "\n",
    "# Load the pre-trained causal language model from Hugging Face's model hub.\n",
    "# The model is automatically distributed across available GPUs if possible using `device_map=\"auto\"`.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",  # Automatically distribute the model across available GPUs\n",
    "    attn_implementation=\"flash_attention_2\",  # Use an optimized attention mechanism for better performance\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 as the default tensor data type for all model parameters\n",
    "    quantization_config=bnb_config  # Apply the defined quantization configuration\n",
    ")\n",
    "\n",
    "# Load the tokenizer associated with the model, enabling fast tokenization.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "tokenizer.padding_side = 'right'  # Set padding to the right to align with model's expectations and prevent warnings\n",
    "\n",
    "# Redefine the pad_token and pad_token_id to use the out-of-vocabulary token (unk_token), \n",
    "# which can help in handling tokens that are not in the tokenizer's vocabulary during processing.\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "\n",
    "# Set up the chat format for the model using a predefined chat template (ChatML) designed for OpenAI API interactions.\n",
    "# This step is useful for ensuring the model and tokenizer are properly configured for generating responses in a chat environment.\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b314719-6db2-497e-878a-09fc578e1868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128258, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaFlashAttention2(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n",
    "# model structure is not complex\n",
    "\n",
    "# Embedding: \n",
    "# The model begins with an embedding layer with 128,258 tokens each mapped to 4,096-dimensional vectors. \n",
    "# This layer converts input token IDs into vectors that the model can process.\n",
    "\n",
    "# Layers: \n",
    "# The core of the model consists of 32 decoder layers stacked sequentially, which are encapsulated within a ModuleList. \n",
    "# Each layer is self-attention block + 2 fully connected layers\n",
    "\n",
    "# LM Head:\n",
    "# A linear layer that maps the final output of the decoder stack back to the vocabulary size (128,258), which is used to predict the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a4445-8ec1-48bf-a1e5-c8da5e3d3297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3211d583-29be-48ce-8a4b-5d3b83f3b1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 1050955776\n"
     ]
    }
   ],
   "source": [
    "# Define a function to count the number of trainable parameters in a model.\n",
    "def count_parameters(model):\n",
    "    # Iterate over all the parameters in the model, summing up the number of elements (numel) for those parameters that are trainable (requires_grad=True).\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Print the total number of trainable parameters in the model. This helps understand the complexity and capacity of the model.\n",
    "print(\"Number of trainable parameters:\", count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac11d4c7-e5da-458f-9ff1-e95ca7967ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1022894422a34468ae6b9bfb391d6393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load jsonl training  data from disk for sql\n",
    "dataset = load_dataset(\"json\", data_files=\"data/train_dataset.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad517560-4176-4389-af4b-e6547e269196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the LoRA settings for model adaptation based on the QLoRA paper and Sebastian Raschka's experiments.\n",
    "# LoRA introduces low-rank matrices that modify the behavior of pre-trained models with minimal additional parameters.\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=128,  # The scaling factor for the learning rate of the LoRA parameters.\n",
    "    lora_dropout=0.05,  # The dropout rate applied to the LoRA weights to prevent overfitting.\n",
    "    r=256,  # The rank of the low-rank matrices. A higher rank increases model capacity and expressivity.\n",
    "    bias=\"none\",  # No bias is used in the LoRA layers to keep the adaptation simpler and more focused on weight adjustments.\n",
    "    target_modules=\"all-linear\",  # Apply LoRA adaptation to all linear modules in the model.\n",
    "    # Alternatively, specify particular modules to apply LoRA using a list, e.g., [\"embed_tokens\", \"lm_head\", \"q_proj\", \"v_proj\"].\n",
    "    task_type=\"CAUSAL_LM\",  # The type of task for which the model is being adapted. Here, it's causal language modeling.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f2e74b1-66de-4dd4-b5f9-556a1ef94e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "\n",
    "    output_dir=\"Meta-Llama-3-8B-text-to-sql-flash-attention-2\",    # directory to save and repository id\n",
    "\n",
    "    num_train_epochs=3,                     # number of training epochs\n",
    "    per_device_train_batch_size=3,          # batch size per device during training\n",
    "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=True,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dc8aca7-6797-4b5e-9dee-e85eb41f3486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a402594940084a7dabeb22f07cf7b431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Define the maximum sequence length for the model's input. This is the longest sequence of tokens the model can process in a single batch.\n",
    "max_seq_length = 3072  # Max sequence length for model input and dataset packing\n",
    "\n",
    "# Instantiate a trainer for supervised fine-tuning (SFT) with specific configurations for LoRA and other settings.\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # The pre-trained model that will be fine-tuned\n",
    "    args=args,  # Training arguments, typically including learning rate, training epochs, etc.\n",
    "    train_dataset=dataset,  # The dataset used for training the model\n",
    "    peft_config=peft_config,  # Parameter-Efficient Fine-Tuning configuration as defined earlier (includes LoRA settings)\n",
    "    max_seq_length=max_seq_length,  # Use the defined maximum sequence length for tokenizing and batching\n",
    "    tokenizer=tokenizer,  # The tokenizer for processing text data into a format suitable for the model\n",
    "    packing=True,  # Enable sequence packing to more efficiently handle variable-length sequences within batches\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # Do not automatically add special tokens (like [CLS], [SEP]), because they are manually templated\n",
    "        \"append_concat_token\": False,  # Avoid adding an extra separator token at the end of sequences\n",
    "        # \"dataset_text_field\": \"text\",  # Uncomment this line to specify which field in the dataset contains the text data if needed\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45415c39-f370-4675-bf41-cd9e37a59b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/kit/tm/px6680/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='168' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [168/168 28:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.928700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.639300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.589900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.573300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.560200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.519900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.488700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.475900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.480400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.480900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.414300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.390800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.395300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.401500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.400800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kit/tm/px6680/miniconda3/envs/huggingface/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kit/tm/px6680/miniconda3/envs/huggingface/lib/python3.9/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/kit/tm/px6680/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/kit/tm/px6680/miniconda3/envs/huggingface/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kit/tm/px6680/miniconda3/envs/huggingface/lib/python3.9/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/kit/tm/px6680/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/kit/tm/px6680/miniconda3/envs/huggingface/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kit/tm/px6680/miniconda3/envs/huggingface/lib/python3.9/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/kit/tm/px6680/miniconda3/envs/huggingface/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kit/tm/px6680/miniconda3/envs/huggingface/lib/python3.9/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "170cdf91-2b9e-41c2-b72a-8b42d29224a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ae6cf-ab8e-4edc-842c-545665562dc3",
   "metadata": {},
   "source": [
    "## Inferece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b91daf88-a450-4ea2-8879-e6fdb461e10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a13c42b4c84001bfc0bc4848b88dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "peft_model_id = \"./Meta-Llama-3-8B-text-to-sql-flash-attention-2\"\n",
    "\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  peft_model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "# load into pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20858238-7df0-46a1-a7d6-4b13f8062993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87af97a6c2554b44892f9b093258a0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kit/tm/px6680/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/kit/tm/px6680/miniconda3/envs/huggingface/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What is the average horizontal bar points for all gymnasts?\n",
      "Original Answer:\n",
      "SELECT AVG(Horizontal_Bar_Points) FROM gymnast\n",
      "Generated Answer:\n",
      "SELECT AVG(Horizontal_Bar_Points) FROM gymnast\n"
     ]
    }
   ],
   "source": [
    "# Load our test dataset\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"data/test_dataset.json\", split=\"train\")\n",
    "rand_idx = randint(0, len(eval_dataset))\n",
    "\n",
    "# Test on sample\n",
    "prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n",
    "\n",
    "print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n",
    "print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\n",
    "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c148d82e-aec1-4c65-87dc-c1c4dbd03900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
