{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T09:21:26.327054700Z",
     "start_time": "2024-07-20T09:21:26.184305500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.evaluation.metrics import aggregate_scores, calculate_scores\n",
    "from src.data.data_loading import load_config\n",
    "from src.data.code_processor import parse_code"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T09:30:48.436445300Z",
     "start_time": "2024-07-20T09:30:48.278168300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "plt.rcParams['font.family'] = 'STIXGeneral'\n",
    "plt.rcParams['font.size'] = 12\n",
    "#%config InlineBackend.figure_format = 'retina'\n",
    "# Set color palette\n",
    "sns.set_palette('Paired')\n",
    "sns.set_context('notebook')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T09:21:32.150658500Z",
     "start_time": "2024-07-20T09:21:32.042111Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set working directory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\merti\\\\PycharmProjects\\\\cadenza-playwright-llm'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set working directory to project root - EXECUTE ONLY ONCE or RESTART KERNEL\n",
    "os.chdir('..')\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T09:21:32.244752800Z",
     "start_time": "2024-07-20T09:21:32.150658500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data + config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "config = load_config(config_path='config/config.yaml')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T09:21:32.345632800Z",
     "start_time": "2024-07-20T09:21:32.244752800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scoring\n",
    "Metrics implemented and used in this project (see `src/evaluation/metrics.py`):\n",
    "* **Weighted BLEU** $ \\in [0.0, 1.0] $: The BLEU score proposed by [Papineni et al. (2002)](https://aclanthology.org/P02-1040.pdf) [1], [2] is a metric that measures the similarity between two sequences of text. The weighted BLEU score is a variant implementd in this project that uses a weighted average of the BLEU scores of the precondition part and the actual generated additional part in teh generated test script. The weights are defined in the configuration file `config/config.yaml`.\n",
    "* **Success Rate** $ \\in [0.0, 1.0] $: The success rate is the proportion of generated test scripts that run successfully, no matter if they are semantically correct or not.\n",
    "* **Levenshtein Distance** $ d(s, t) \\in \\mathbb{N} $: The Levenshtein distance between strings $ s $ and $ t $ is an integer that measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to change $ s $ into $ t $."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Manual Evaluation**\n",
    "First we are going to test the core functionality of the scoring functions by defining some example test cases and running the scoring functions on them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Define example test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        'test_case': '1',\n",
    "        'test_step': '2',\n",
    "        'generated_code': parse_code(config['paths']['prediction_dir']+'/1_2.spec.ts'),\n",
    "        'validation_code': parse_code(config['dataloading']['test_script_dir']+'/1_2.spec.ts'),\n",
    "        'precondition_code': parse_code(config['dataloading']['test_script_dir']+'/1_1.spec.ts')\n",
    "    },\n",
    "    {\n",
    "        'test_case': '2',\n",
    "        'test_step': '2',\n",
    "        'generated_code': parse_code(config['paths']['prediction_dir']+'/2_2.spec.ts'),\n",
    "        'validation_code': parse_code(config['dataloading']['test_script_dir']+'/2_2.spec.ts'),\n",
    "        'precondition_code': parse_code(config['dataloading']['test_script_dir']+'/2_1.spec.ts')\n",
    "    }\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T09:21:32.510125500Z",
     "start_time": "2024-07-20T09:21:32.345632800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "\"import { test, expect } from '@playwright/test';\\nimport { writeFileSync } from 'fs';\\n\\ntest('test', async ({ page }) => {\\n  await page.goto('http://localhost:8080/cadenza/');\\n  await page.getByRole('link', { name: 'Anmelden' }).click();\\n  await page.getByLabel('Benutzername *').click();\\n  await page.getByLabel('Benutzername *').fill('Admin');\\n  await page.getByRole('button', { name: 'Anmelden' }).click();\\n  await page.getByTestId('create-workbook-button').click();\\n});\\n\""
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cases[1]['generated_code']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T09:21:32.605299600Z",
     "start_time": "2024-07-20T09:21:32.510125500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-20 11:21:32 [\u001B[34msrc.evaluation.metrics:86\u001B[0m] [DEBUG\u001B[0m] >>>> Calculating scores for test case 1_2...\u001B[0m\n",
      "data\\temp\\eval_run\\screenshots\n",
      "2024-07-20 11:21:32 [\u001B[34msrc.evaluation.metrics:209\u001B[0m] [DEBUG\u001B[0m] >>>> Current working directory: C:\\Users\\merti\\PycharmProjects\\cadenza-playwright-llm\u001B[0m\n",
      "2024-07-20 11:21:32 [\u001B[34msrc.evaluation.metrics:210\u001B[0m] [DEBUG\u001B[0m] >>>> Screenshot directory: data\\temp\\eval_run\\screenshots\u001B[0m\n",
      "2024-07-20 11:21:32 [\u001B[34msrc.evaluation.metrics:211\u001B[0m] [DEBUG\u001B[0m] >>>> Evaluation run directory: data\\temp\\eval_run\u001B[0m\n",
      "2024-07-20 11:21:32 [\u001B[34msrc.evaluation.metrics:219\u001B[0m] [DEBUG\u001B[0m] >>>> Screenshot path: data/temp/eval_run/screenshots/1_2.png\u001B[0m\n",
      "2024-07-20 11:21:32 [\u001B[34msrc.evaluation.metrics:258\u001B[0m] [DEBUG\u001B[0m] >>>> Created temp directory: data\\temp\\eval_run\\test_script\u001B[0m\n",
      "2024-07-20 11:21:32 [\u001B[34msrc.evaluation.metrics:261\u001B[0m] [DEBUG\u001B[0m] >>>> Temp file path: data\\temp\\eval_run\\test_script\\1_2.spec.ts\u001B[0m\n",
      "2024-07-20 11:21:32 [\u001B[34msrc.evaluation.metrics:267\u001B[0m] [DEBUG\u001B[0m] >>>> File data\\temp\\eval_run\\test_script\\1_2.spec.ts created successfully.\u001B[0m\n",
      "2024-07-20 11:21:33 [\u001B[34msrc.evaluation.metrics:277\u001B[0m] [DEBUG\u001B[0m] >>>> Current working directory: C:\\Users\\merti\\PycharmProjects\\cadenza-playwright-llm\u001B[0m\n",
      "2024-07-20 11:21:51 [\u001B[34msrc.evaluation.metrics:292\u001B[0m] [DEBUG\u001B[0m] >>>> Playwright test result: 1\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\merti\\anaconda3\\envs\\uitest\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\merti\\anaconda3\\envs\\uitest\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\merti/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:04<00:00, 10.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-20 11:21:56 [\u001B[34msrc.evaluation.metrics:86\u001B[0m] [DEBUG\u001B[0m] >>>> Calculating scores for test case 2_2...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\temp\\eval_run\\screenshots\n",
      "2024-07-20 11:21:56 [\u001B[34msrc.evaluation.metrics:209\u001B[0m] [DEBUG\u001B[0m] >>>> Current working directory: C:\\Users\\merti\\PycharmProjects\\cadenza-playwright-llm\u001B[0m\n",
      "2024-07-20 11:21:56 [\u001B[34msrc.evaluation.metrics:210\u001B[0m] [DEBUG\u001B[0m] >>>> Screenshot directory: data\\temp\\eval_run\\screenshots\u001B[0m\n",
      "2024-07-20 11:21:56 [\u001B[34msrc.evaluation.metrics:211\u001B[0m] [DEBUG\u001B[0m] >>>> Evaluation run directory: data\\temp\\eval_run\u001B[0m\n",
      "2024-07-20 11:21:56 [\u001B[34msrc.evaluation.metrics:219\u001B[0m] [DEBUG\u001B[0m] >>>> Screenshot path: data/temp/eval_run/screenshots/2_2.png\u001B[0m\n",
      "2024-07-20 11:21:56 [\u001B[34msrc.evaluation.metrics:258\u001B[0m] [DEBUG\u001B[0m] >>>> Created temp directory: data\\temp\\eval_run\\test_script\u001B[0m\n",
      "2024-07-20 11:21:56 [\u001B[34msrc.evaluation.metrics:261\u001B[0m] [DEBUG\u001B[0m] >>>> Temp file path: data\\temp\\eval_run\\test_script\\2_2.spec.ts\u001B[0m\n",
      "2024-07-20 11:21:56 [\u001B[34msrc.evaluation.metrics:267\u001B[0m] [DEBUG\u001B[0m] >>>> File data\\temp\\eval_run\\test_script\\2_2.spec.ts created successfully.\u001B[0m\n",
      "2024-07-20 11:21:57 [\u001B[34msrc.evaluation.metrics:277\u001B[0m] [DEBUG\u001B[0m] >>>> Current working directory: C:\\Users\\merti\\PycharmProjects\\cadenza-playwright-llm\u001B[0m\n",
      "2024-07-20 11:21:59 [\u001B[34msrc.evaluation.metrics:292\u001B[0m] [DEBUG\u001B[0m] >>>> Playwright test result: 1\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'weighted bleu': [0.5006969523919225, 0.3988451029814817],\n 'success rate': [1, 1],\n 'levenshtein distance': [0.09230769230769231, 0.2632398753894081],\n 'similarity': [0, 0]}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run scoring for test cases\n",
    "scores = calculate_scores(test_cases, config)\n",
    "scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T09:21:59.961911600Z",
     "start_time": "2024-07-20T09:21:32.605299600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-20 11:22:00 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'weighted bleu': 0.4497710276867021,\n 'success rate': 1.0,\n 'levenshtein distance': 0.1777737838485502,\n 'similarity': None}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate scores\n",
    "agg_scores = aggregate_scores(scores)\n",
    "agg_scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T09:22:00.066718Z",
     "start_time": "2024-07-20T09:21:59.961911600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Automated Evaluation**\n",
    "For simplicity an evaluation test script is implemented in `scripts/evaluation.py` that runs the scoring functions on all test cases available in the prediction directory defined in the configuration file `config/config.yaml`. The evaluation results are also automatically saved as a pickle file in the scoring results directory defined in the configuration file `config/config.yaml`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-20 11:22:05 [\u001B[34m__main__:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 11:22:05 [\u001B[34m__main__:43\u001B[0m] [DEBUG\u001B[0m] >>>> Test cases: [{'test_case': '1', 'test_step': '2', 'generated_code': \"import { test, expect } from '@playwright/test';\\nimport { writeFileSync } from 'fs';\\n\\ntest('test', async ({ page }) => {\\n  await page.goto('http://localhost:8080/cadenza/');\\n  await page.getByRole('link', { name: 'Anmelden' }).click();\\n  await page.getByLabel('Benutzername *').click();\\n  await page.getByLabel('Benutzername *').fill('Admin');\\n  await page.getByLabel('Benutzername *').press('Tab');\\n  await page.getByPlaceholder(' ').fill('Admin');\\n  await page.getByRole('button', { name: 'Anmelden' }).click();\\n  await page.getByText('Verzeichnis Gewässergüte', { exact: true }).click();\\n  const parentElement = await page.getByText('Arbeitsmappe Übersicht Messstellen').locator('..');\\n  await parentElement.locator('.d-icon.d-icon-bold.status-icon').click();\\n});\\n\", 'validation_code': \"import { test, expect } from '@playwright/test';\\nimport { writeFileSync } from 'fs';\\ntest('test', async ({ page }) => {\\n  await page.goto('http://localhost:8080/cadenza/');\\n  await page.getByRole('link', { name: 'Anmelden' }).click();\\n  await page.getByLabel('Benutzername *').click();\\n  await page.getByLabel('Benutzername *').fill('Admin');\\n  await page.getByLabel('Benutzername *').press('Tab');\\n  await page.getByPlaceholder(' ').fill('Admin');\\n  await page.getByRole('button', { name: 'Anmelden' }).click();\\n  await page.getByText('Verzeichnis Gewässergüte', { exact: true }).click();\\n  const parentElement = await page.getByText('Arbeitsmappe Übersicht Messstellen').locator('..');\\n  await parentElement.locator('.d-icon.d-icon-bold.status-icon').click(); \\n  await page.getByRole('link', { name: 'Tabelle Messstellenliste' }).click();\\n\\n});\", 'precondition_code': \"import { test, expect } from '@playwright/test';\\nimport { writeFileSync } from 'fs';\\n\\n\\ntest('test', async ({ page }) => {\\n  await page.goto('http://localhost:8080/cadenza/');\\n  await page.getByRole('link', { name: 'Anmelden' }).click();\\n  await page.getByLabel('Benutzername *').click();\\n  await page.getByLabel('Benutzername *').fill('Admin');\\n  await page.getByLabel('Benutzername *').press('Tab');\\n  await page.getByPlaceholder(' ').fill('Admin');\\n  await page.getByRole('button', { name: 'Anmelden' }).click();\\n  await page.getByText('Verzeichnis Gewässergüte', { exact: true }).click();\\n  const parentElement = await page.getByText('Arbeitsmappe Übersicht Messstellen').locator('..');\\n  await parentElement.locator('.d-icon.d-icon-bold.status-icon').click(); \\n\\n});\"}, {'test_case': '2', 'test_step': '2', 'generated_code': \"import { test, expect } from '@playwright/test';\\nimport { writeFileSync } from 'fs';\\n\\ntest('test', async ({ page }) => {\\n  await page.goto('http://localhost:8080/cadenza/');\\n  await page.getByRole('link', { name: 'Anmelden' }).click();\\n  await page.getByLabel('Benutzername *').click();\\n  await page.getByLabel('Benutzername *').fill('Admin');\\n  await page.getByRole('button', { name: 'Anmelden' }).click();\\n  await page.getByTestId('create-workbook-button').click();\\n});\\n\", 'validation_code': \"import { test, expect } from '@playwright/test';\\nimport { writeFileSync } from 'fs';\\ntest('test', async ({ page }) => {\\n  await page.goto('http://localhost:8080/cadenza/');\\n  await page.getByRole('link', { name: 'Anmelden' }).click();\\n  await page.getByLabel('Benutzername *').click();\\n  await page.getByLabel('Benutzername *').fill('Admin');\\n  await page.getByLabel('Benutzername *').press('Tab');\\n  await page.getByPlaceholder(' ').fill('Admin');\\n  await page.getByRole('button', { name: 'Anmelden' }).click();\\n  await page.getByTestId('create-workbook-button').click();\\n  await page.getByRole('menuitem', { name: 'Gewässer' }).click();\\n});\", 'precondition_code': \"import { test, expect } from '@playwright/test';\\nimport { writeFileSync } from 'fs';\\ntest('test', async ({ page }) => {\\n  await page.goto('http://localhost:8080/cadenza/');\\n  await page.getByRole('link', { name: 'Anmelden' }).click();\\n  await page.getByLabel('Benutzername *').click();\\n  await page.getByLabel('Benutzername *').fill('Admin');\\n  await page.getByLabel('Benutzername *').press('Tab');\\n  await page.getByPlaceholder(' ').fill('Admin');\\n  await page.getByRole('button', { name: 'Anmelden' }).click();\\n  await page.getByTestId('create-workbook-button').click();\\n\\n});\"}]\u001B[0m\n",
      "2024-07-20 11:22:05 [\u001B[34msrc.evaluation.metrics:86\u001B[0m] [DEBUG\u001B[0m] >>>> Calculating scores for test case 1_2...\u001B[0m\n",
      "data\\temp\\eval_run\\screenshots\n",
      "2024-07-20 11:22:05 [\u001B[34msrc.evaluation.metrics:209\u001B[0m] [DEBUG\u001B[0m] >>>> Current working directory: C:\\Users\\merti\\PycharmProjects\\cadenza-playwright-llm\u001B[0m\n",
      "2024-07-20 11:22:05 [\u001B[34msrc.evaluation.metrics:210\u001B[0m] [DEBUG\u001B[0m] >>>> Screenshot directory: data\\temp\\eval_run\\screenshots\u001B[0m\n",
      "2024-07-20 11:22:05 [\u001B[34msrc.evaluation.metrics:211\u001B[0m] [DEBUG\u001B[0m] >>>> Evaluation run directory: data\\temp\\eval_run\u001B[0m\n",
      "2024-07-20 11:22:05 [\u001B[34msrc.evaluation.metrics:219\u001B[0m] [DEBUG\u001B[0m] >>>> Screenshot path: data/temp/eval_run/screenshots/1_2.png\u001B[0m\n",
      "2024-07-20 11:22:05 [\u001B[34msrc.evaluation.metrics:258\u001B[0m] [DEBUG\u001B[0m] >>>> Created temp directory: data\\temp\\eval_run\\test_script\u001B[0m\n",
      "2024-07-20 11:22:05 [\u001B[34msrc.evaluation.metrics:261\u001B[0m] [DEBUG\u001B[0m] >>>> Temp file path: data\\temp\\eval_run\\test_script\\1_2.spec.ts\u001B[0m\n",
      "2024-07-20 11:22:05 [\u001B[34msrc.evaluation.metrics:267\u001B[0m] [DEBUG\u001B[0m] >>>> File data\\temp\\eval_run\\test_script\\1_2.spec.ts created successfully.\u001B[0m\n",
      "2024-07-20 11:22:06 [\u001B[34msrc.evaluation.metrics:277\u001B[0m] [DEBUG\u001B[0m] >>>> Current working directory: C:\\Users\\merti\\PycharmProjects\\cadenza-playwright-llm\u001B[0m\n",
      "Error: No tests found.\n",
      "Make sure that arguments are regular expressions matching test files.\n",
      "You may need to escape symbols like \"$\" or \"*\" and quote the arguments.\n",
      "\n",
      "2024-07-20 11:22:08 [\u001B[34msrc.evaluation.metrics:292\u001B[0m] [DEBUG\u001B[0m] >>>> Playwright test result: 1\u001B[0m\n",
      "2024-07-20 11:22:08 [\u001B[34msrc.evaluation.metrics:86\u001B[0m] [DEBUG\u001B[0m] >>>> Calculating scores for test case 2_2...\u001B[0m\n",
      "data\\temp\\eval_run\\screenshots\n",
      "2024-07-20 11:22:08 [\u001B[34msrc.evaluation.metrics:209\u001B[0m] [DEBUG\u001B[0m] >>>> Current working directory: C:\\Users\\merti\\PycharmProjects\\cadenza-playwright-llm\u001B[0m\n",
      "2024-07-20 11:22:08 [\u001B[34msrc.evaluation.metrics:210\u001B[0m] [DEBUG\u001B[0m] >>>> Screenshot directory: data\\temp\\eval_run\\screenshots\u001B[0m\n",
      "2024-07-20 11:22:08 [\u001B[34msrc.evaluation.metrics:211\u001B[0m] [DEBUG\u001B[0m] >>>> Evaluation run directory: data\\temp\\eval_run\u001B[0m\n",
      "2024-07-20 11:22:08 [\u001B[34msrc.evaluation.metrics:219\u001B[0m] [DEBUG\u001B[0m] >>>> Screenshot path: data/temp/eval_run/screenshots/2_2.png\u001B[0m\n",
      "2024-07-20 11:22:08 [\u001B[34msrc.evaluation.metrics:258\u001B[0m] [DEBUG\u001B[0m] >>>> Created temp directory: data\\temp\\eval_run\\test_script\u001B[0m\n",
      "2024-07-20 11:22:08 [\u001B[34msrc.evaluation.metrics:261\u001B[0m] [DEBUG\u001B[0m] >>>> Temp file path: data\\temp\\eval_run\\test_script\\2_2.spec.ts\u001B[0m\n",
      "2024-07-20 11:22:08 [\u001B[34msrc.evaluation.metrics:267\u001B[0m] [DEBUG\u001B[0m] >>>> File data\\temp\\eval_run\\test_script\\2_2.spec.ts created successfully.\u001B[0m\n",
      "2024-07-20 11:22:09 [\u001B[34msrc.evaluation.metrics:277\u001B[0m] [DEBUG\u001B[0m] >>>> Current working directory: C:\\Users\\merti\\PycharmProjects\\cadenza-playwright-llm\u001B[0m\n",
      "Error: No tests found.\n",
      "Make sure that arguments are regular expressions matching test files.\n",
      "You may need to escape symbols like \"$\" or \"*\" and quote the arguments.\n",
      "\n",
      "2024-07-20 11:22:11 [\u001B[34msrc.evaluation.metrics:292\u001B[0m] [DEBUG\u001B[0m] >>>> Playwright test result: 1\u001B[0m\n",
      "2024-07-20 11:22:11 [\u001B[34m__main__:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 11:22:11 [\u001B[34m__main__:47\u001B[0m] [DEBUG\u001B[0m] >>>> Scores: {'weighted bleu': [0.5006969523919225, 0.3988451029814817], 'success rate': [1, 1], 'levenshtein distance': [0.09230769230769231, 0.2632398753894081], 'similarity': [0, 0]}\u001B[0m\n",
      "2024-07-20 11:22:11 [\u001B[34m__main__:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 11:22:11 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 11:22:11 [\u001B[34m__main__:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.4497710276867021, 'success rate': 1.0, 'levenshtein distance': 0.1777737838485502, 'similarity': None}\u001B[0m\n",
      "2024-07-20 11:22:11 [\u001B[34m__main__:65\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-112211.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\merti\\anaconda3\\envs\\uitest\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\merti\\anaconda3\\envs\\uitest\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Run complete automated evaluation script\n",
    "!python scripts/evaluation.py --config=config/config.yaml"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T09:22:12.136061800Z",
     "start_time": "2024-07-20T09:22:00.066718Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is also possible to evaluate the test cases inside the notebook by using the evaluate_test_cases() function without running the whole script but directly calling the main function of the script:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-20 11:22:12 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 11:22:12 [\u001B[34mscripts.evaluation:43\u001B[0m] [DEBUG\u001B[0m] >>>> Test cases: [{'test_case': '1', 'test_step': '2', 'generated_code': \"import { test, expect } from '@playwright/test';\\nimport { writeFileSync } from 'fs';\\n\\ntest('test', async ({ page }) => {\\n  await page.goto('http://localhost:8080/cadenza/');\\n  await page.getByRole('link', { name: 'Anmelden' }).click();\\n  await page.getByLabel('Benutzername *').click();\\n  await page.getByLabel('Benutzername *').fill('Admin');\\n  await page.getByLabel('Benutzername *').press('Tab');\\n  await page.getByPlaceholder(' ').fill('Admin');\\n  await page.getByRole('button', { name: 'Anmelden' }).click();\\n  await page.getByText('Verzeichnis Gewässergüte', { exact: true }).click();\\n  const parentElement = await page.getByText('Arbeitsmappe Übersicht Messstellen').locator('..');\\n  await parentElement.locator('.d-icon.d-icon-bold.status-icon').click();\\n});\\n\", 'validation_code': \"import { test, expect } from '@playwright/test';\\nimport { writeFileSync } from 'fs';\\ntest('test', async ({ page }) => {\\n  await page.goto('http://localhost:8080/cadenza/');\\n  await page.getByRole('link', { name: 'Anmelden' }).click();\\n  await page.getByLabel('Benutzername *').click();\\n  await page.getByLabel('Benutzername *').fill('Admin');\\n  await page.getByLabel('Benutzername *').press('Tab');\\n  await page.getByPlaceholder(' ').fill('Admin');\\n  await page.getByRole('button', { name: 'Anmelden' }).click();\\n  await page.getByText('Verzeichnis Gewässergüte', { exact: true }).click();\\n  const parentElement = await page.getByText('Arbeitsmappe Übersicht Messstellen').locator('..');\\n  await parentElement.locator('.d-icon.d-icon-bold.status-icon').click(); \\n  await page.getByRole('link', { name: 'Tabelle Messstellenliste' }).click();\\n\\n});\", 'precondition_code': \"import { test, expect } from '@playwright/test';\\nimport { writeFileSync } from 'fs';\\n\\n\\ntest('test', async ({ page }) => {\\n  await page.goto('http://localhost:8080/cadenza/');\\n  await page.getByRole('link', { name: 'Anmelden' }).click();\\n  await page.getByLabel('Benutzername *').click();\\n  await page.getByLabel('Benutzername *').fill('Admin');\\n  await page.getByLabel('Benutzername *').press('Tab');\\n  await page.getByPlaceholder(' ').fill('Admin');\\n  await page.getByRole('button', { name: 'Anmelden' }).click();\\n  await page.getByText('Verzeichnis Gewässergüte', { exact: true }).click();\\n  const parentElement = await page.getByText('Arbeitsmappe Übersicht Messstellen').locator('..');\\n  await parentElement.locator('.d-icon.d-icon-bold.status-icon').click(); \\n\\n});\"}, {'test_case': '2', 'test_step': '2', 'generated_code': \"import { test, expect } from '@playwright/test';\\nimport { writeFileSync } from 'fs';\\n\\ntest('test', async ({ page }) => {\\n  await page.goto('http://localhost:8080/cadenza/');\\n  await page.getByRole('link', { name: 'Anmelden' }).click();\\n  await page.getByLabel('Benutzername *').click();\\n  await page.getByLabel('Benutzername *').fill('Admin');\\n  await page.getByRole('button', { name: 'Anmelden' }).click();\\n  await page.getByTestId('create-workbook-button').click();\\n});\\n\", 'validation_code': \"import { test, expect } from '@playwright/test';\\nimport { writeFileSync } from 'fs';\\ntest('test', async ({ page }) => {\\n  await page.goto('http://localhost:8080/cadenza/');\\n  await page.getByRole('link', { name: 'Anmelden' }).click();\\n  await page.getByLabel('Benutzername *').click();\\n  await page.getByLabel('Benutzername *').fill('Admin');\\n  await page.getByLabel('Benutzername *').press('Tab');\\n  await page.getByPlaceholder(' ').fill('Admin');\\n  await page.getByRole('button', { name: 'Anmelden' }).click();\\n  await page.getByTestId('create-workbook-button').click();\\n  await page.getByRole('menuitem', { name: 'Gewässer' }).click();\\n});\", 'precondition_code': \"import { test, expect } from '@playwright/test';\\nimport { writeFileSync } from 'fs';\\ntest('test', async ({ page }) => {\\n  await page.goto('http://localhost:8080/cadenza/');\\n  await page.getByRole('link', { name: 'Anmelden' }).click();\\n  await page.getByLabel('Benutzername *').click();\\n  await page.getByLabel('Benutzername *').fill('Admin');\\n  await page.getByLabel('Benutzername *').press('Tab');\\n  await page.getByPlaceholder(' ').fill('Admin');\\n  await page.getByRole('button', { name: 'Anmelden' }).click();\\n  await page.getByTestId('create-workbook-button').click();\\n\\n});\"}]\u001B[0m\n",
      "2024-07-20 11:22:12 [\u001B[34msrc.evaluation.metrics:86\u001B[0m] [DEBUG\u001B[0m] >>>> Calculating scores for test case 1_2...\u001B[0m\n",
      "data\\temp\\eval_run\\screenshots\n",
      "2024-07-20 11:22:12 [\u001B[34msrc.evaluation.metrics:209\u001B[0m] [DEBUG\u001B[0m] >>>> Current working directory: C:\\Users\\merti\\PycharmProjects\\cadenza-playwright-llm\u001B[0m\n",
      "2024-07-20 11:22:12 [\u001B[34msrc.evaluation.metrics:210\u001B[0m] [DEBUG\u001B[0m] >>>> Screenshot directory: data\\temp\\eval_run\\screenshots\u001B[0m\n",
      "2024-07-20 11:22:12 [\u001B[34msrc.evaluation.metrics:211\u001B[0m] [DEBUG\u001B[0m] >>>> Evaluation run directory: data\\temp\\eval_run\u001B[0m\n",
      "2024-07-20 11:22:12 [\u001B[34msrc.evaluation.metrics:219\u001B[0m] [DEBUG\u001B[0m] >>>> Screenshot path: data/temp/eval_run/screenshots/1_2.png\u001B[0m\n",
      "2024-07-20 11:22:12 [\u001B[34msrc.evaluation.metrics:258\u001B[0m] [DEBUG\u001B[0m] >>>> Created temp directory: data\\temp\\eval_run\\test_script\u001B[0m\n",
      "2024-07-20 11:22:12 [\u001B[34msrc.evaluation.metrics:261\u001B[0m] [DEBUG\u001B[0m] >>>> Temp file path: data\\temp\\eval_run\\test_script\\1_2.spec.ts\u001B[0m\n",
      "2024-07-20 11:22:12 [\u001B[34msrc.evaluation.metrics:267\u001B[0m] [DEBUG\u001B[0m] >>>> File data\\temp\\eval_run\\test_script\\1_2.spec.ts created successfully.\u001B[0m\n",
      "2024-07-20 11:22:13 [\u001B[34msrc.evaluation.metrics:277\u001B[0m] [DEBUG\u001B[0m] >>>> Current working directory: C:\\Users\\merti\\PycharmProjects\\cadenza-playwright-llm\u001B[0m\n",
      "2024-07-20 11:22:15 [\u001B[34msrc.evaluation.metrics:292\u001B[0m] [DEBUG\u001B[0m] >>>> Playwright test result: 1\u001B[0m\n",
      "2024-07-20 11:22:15 [\u001B[34msrc.evaluation.metrics:86\u001B[0m] [DEBUG\u001B[0m] >>>> Calculating scores for test case 2_2...\u001B[0m\n",
      "data\\temp\\eval_run\\screenshots\n",
      "2024-07-20 11:22:15 [\u001B[34msrc.evaluation.metrics:209\u001B[0m] [DEBUG\u001B[0m] >>>> Current working directory: C:\\Users\\merti\\PycharmProjects\\cadenza-playwright-llm\u001B[0m\n",
      "2024-07-20 11:22:15 [\u001B[34msrc.evaluation.metrics:210\u001B[0m] [DEBUG\u001B[0m] >>>> Screenshot directory: data\\temp\\eval_run\\screenshots\u001B[0m\n",
      "2024-07-20 11:22:15 [\u001B[34msrc.evaluation.metrics:211\u001B[0m] [DEBUG\u001B[0m] >>>> Evaluation run directory: data\\temp\\eval_run\u001B[0m\n",
      "2024-07-20 11:22:15 [\u001B[34msrc.evaluation.metrics:219\u001B[0m] [DEBUG\u001B[0m] >>>> Screenshot path: data/temp/eval_run/screenshots/2_2.png\u001B[0m\n",
      "2024-07-20 11:22:15 [\u001B[34msrc.evaluation.metrics:258\u001B[0m] [DEBUG\u001B[0m] >>>> Created temp directory: data\\temp\\eval_run\\test_script\u001B[0m\n",
      "2024-07-20 11:22:15 [\u001B[34msrc.evaluation.metrics:261\u001B[0m] [DEBUG\u001B[0m] >>>> Temp file path: data\\temp\\eval_run\\test_script\\2_2.spec.ts\u001B[0m\n",
      "2024-07-20 11:22:15 [\u001B[34msrc.evaluation.metrics:267\u001B[0m] [DEBUG\u001B[0m] >>>> File data\\temp\\eval_run\\test_script\\2_2.spec.ts created successfully.\u001B[0m\n",
      "2024-07-20 11:22:16 [\u001B[34msrc.evaluation.metrics:277\u001B[0m] [DEBUG\u001B[0m] >>>> Current working directory: C:\\Users\\merti\\PycharmProjects\\cadenza-playwright-llm\u001B[0m\n",
      "2024-07-20 11:22:18 [\u001B[34msrc.evaluation.metrics:292\u001B[0m] [DEBUG\u001B[0m] >>>> Playwright test result: 1\u001B[0m\n",
      "2024-07-20 11:22:18 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 11:22:18 [\u001B[34mscripts.evaluation:47\u001B[0m] [DEBUG\u001B[0m] >>>> Scores: {'weighted bleu': [0.5006969523919225, 0.3988451029814817], 'success rate': [1, 1], 'levenshtein distance': [0.09230769230769231, 0.2632398753894081], 'similarity': [0, 0]}\u001B[0m\n",
      "2024-07-20 11:22:18 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 11:22:18 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 11:22:18 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.4497710276867021, 'success rate': 1.0, 'levenshtein distance': 0.1777737838485502, 'similarity': None}\u001B[0m\n",
      "2024-07-20 11:22:18 [\u001B[34mscripts.evaluation:65\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-112218.pkl\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "  file_id test_case test_step  weighted bleu  success rate  \\\n0     1_2         1         2       0.500697             1   \n1     2_2         2         2       0.398845             1   \n\n   levenshtein distance  similarity  \n0              0.092308           0  \n1              0.263240           0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_id</th>\n      <th>test_case</th>\n      <th>test_step</th>\n      <th>weighted bleu</th>\n      <th>success rate</th>\n      <th>levenshtein distance</th>\n      <th>similarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1_2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.500697</td>\n      <td>1</td>\n      <td>0.092308</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2_2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.398845</td>\n      <td>1</td>\n      <td>0.263240</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run complete automated evaluation inside notebook\n",
    "from scripts.evaluation import evaluate_test_cases\n",
    "results = evaluate_test_cases(config)\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T09:22:18.708951400Z",
     "start_time": "2024-07-20T09:22:12.141072100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run Scoring on all Different Predictions\n",
    "We generated different predictions using different input configurations. Now we want to evaluate all of them and compare the results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Adjust file names from pred.ts to spec.ts\n",
    "def rename_files_in_directory(directory: str):\n",
    "    # List all files in the directory\n",
    "    for file_name in os.listdir(directory):\n",
    "        # Check if the file name ends with '.pred.ts'\n",
    "        if file_name.endswith('.pred.ts'):\n",
    "            # Create the new file name\n",
    "            new_file_name = file_name.replace('.pred.ts', '.spec.ts')\n",
    "            # Create the full paths to the old and new file names\n",
    "            old_file_path = os.path.join(directory, file_name)\n",
    "            new_file_path = os.path.join(directory, new_file_name)\n",
    "            # Rename the file\n",
    "            os.rename(old_file_path, new_file_path)\n",
    "            print(f\"Renamed '{old_file_path}' to '{new_file_path}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T09:33:38.479112400Z",
     "start_time": "2024-07-20T09:33:38.344372200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script'...\n",
      "2024-07-20 12:11:06 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 12:11:12 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 12:11:12 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 12:11:12 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 12:11:12 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.4497710276867021, 'success rate': 1.0, 'levenshtein distance': 0.1777737838485502, 'similarity': None}\u001B[0m\n",
      "2024-07-20 12:11:12 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-121112.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:05<01:08,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_finetuned_T1_sc+_html+_single'...\n",
      "2024-07-20 12:11:12 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 12:11:22 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 10_4: Line 29: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:11:38 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 11_3: Line 28: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:11:44 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 12_1: Line 22: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:11:50 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 12_3: Line 24: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:12:10 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 14_1: Line 29: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:12:25 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 15_3: Line 24: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:12:32 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 15_5: Line 26: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:12:35 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 16_1: Line 29: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:12:41 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 16_3: Line 25: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:12:54 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 17_4: Line 25: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:12:57 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 18_1: Line 29: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:13:04 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 18_3: Line 25: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:13:16 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 19_4: Line 29: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:13:19 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 1_1: Line 29: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:13:32 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 25_1: Line 22: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:13:38 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 25_3: Line 25: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:13:42 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 25_4: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:13:48 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 26_1: Line 29: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:14:22 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 28_1: Line 29: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:14:54 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 3_1: Line 27: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:15:00 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 4_1: Line 27: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:15:22 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 5_1: Line 29: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:15:51 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 8_1: Line 29: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:15:57 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 8_3: Line 15: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:16:01 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 8_4: Line 25: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:16:26 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 9_6: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:16:29 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 12:16:29 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 12:16:29 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 12:16:29 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 12:16:29 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.4616768978746557, 'success rate': 1.0, 'levenshtein distance': 0.31445282118052675, 'similarity': None}\u001B[0m\n",
      "2024-07-20 12:16:29 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-121629.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [05:22<49:04, 133.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_finetuned_T1_sc+_html+_single_test_set'...\n",
      "2024-07-20 12:16:29 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 12:16:36 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 15_3: Line 24: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:16:42 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 15_5: Line 26: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:16:45 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 28_1: Line 29: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:17:32 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 9_6: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:17:35 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 12:17:35 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 12:17:35 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 12:17:35 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 12:17:35 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.49704073849331987, 'success rate': 1.0, 'levenshtein distance': 0.3390701850912101, 'similarity': None}\u001B[0m\n",
      "2024-07-20 12:17:35 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-121735.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [06:29<38:00, 108.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_finetuned_T1_sc-_html+_single'...\n",
      "2024-07-20 12:17:35 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 12:18:13 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 12_3: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:18:16 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 12_4: Line 21: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:19:49 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 1_3: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:21:36 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 3_2: Line 21: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:23:16 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 12:23:16 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 12:23:16 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 12:23:16 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 12:23:16 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.765972240622647, 'success rate': 1.0, 'levenshtein distance': 0.0726310623862712, 'similarity': None}\u001B[0m\n",
      "2024-07-20 12:23:16 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-122316.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [12:09<1:03:05, 189.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_finetuned_T1_sc-_html+_single_test_set'...\n",
      "2024-07-20 12:23:16 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 12:24:17 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 12:24:17 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 12:24:17 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 12:24:17 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.7972442574430477, 'success rate': 1.0, 'levenshtein distance': 0.05943636180917157, 'similarity': None}\u001B[0m\n",
      "2024-07-20 12:24:17 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-122417.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [13:11<46:29, 146.82s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_finetuned_T5_sc+_html+_single'...\n",
      "2024-07-20 12:24:17 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 12:24:18 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 10_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:24:27 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 10_4: Line 25: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:24:41 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 11_3: Line 26: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:25:01 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 13_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:25:19 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 15_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:25:43 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 17_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:26:04 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 19_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:26:15 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 1_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:26:42 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 26_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:27:15 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 28_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:27:45 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 3_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:27:50 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 4_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:28:11 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 5_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:28:26 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 7_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:28:38 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 8_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:28:50 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 8_5: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:28:56 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 9_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:29:13 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 12:29:13 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 12:29:13 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 12:29:13 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 12:29:13 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.5428070030272967, 'success rate': 1.0, 'levenshtein distance': 0.20876644679554537, 'similarity': None}\u001B[0m\n",
      "2024-07-20 12:29:13 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-122913.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [18:06<58:25, 194.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_finetuned_T5_sc+_html+_single_test_set'...\n",
      "2024-07-20 12:29:13 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 12:29:13 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 15_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:29:28 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 28_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:29:59 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 9_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:30:16 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 12:30:16 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 12:30:16 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 12:30:16 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 12:30:16 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.5654742435618512, 'success rate': 1.0, 'levenshtein distance': 0.2348513794313099, 'similarity': None}\u001B[0m\n",
      "2024-07-20 12:30:16 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-123016.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [19:10<43:26, 153.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_finetuned_T5_sc-_html+_single'...\n",
      "2024-07-20 12:30:16 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 12:33:42 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 30_3: Line 26: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:33:48 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 3_1: Line 11: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:33:51 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 3_2: Line 21: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:35:17 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 12:35:17 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 12:35:17 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 12:35:17 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 12:35:17 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.7712575379809627, 'success rate': 1.0, 'levenshtein distance': 0.06291960074643652, 'similarity': None}\u001B[0m\n",
      "2024-07-20 12:35:17 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-123517.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [24:10<53:02, 198.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_finetuned_T5_sc-_html+_single_test_set'...\n",
      "2024-07-20 12:35:17 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 12:35:56 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 30_3: Line 26: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:36:19 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 12:36:19 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 12:36:19 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 12:36:19 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 12:36:19 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.7545435429993087, 'success rate': 1.0, 'levenshtein distance': 0.06334898614530313, 'similarity': None}\u001B[0m\n",
      "2024-07-20 12:36:19 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-123619.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [25:12<39:15, 157.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_pretr_T1_sc+_html+_all'...\n",
      "2024-07-20 12:36:19 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 12:36:25 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 10_3: Line 28: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:36:51 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 12_2: Line 37: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:37:38 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 16_2: Line 42: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:37:41 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 16_3: Line 26: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:37:59 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 18_2: Line 46: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:38:02 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 18_3: Line 45: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:39:04 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 27_1: Line 27: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:39:57 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 30_2: Line 26: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:40:06 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 3_1: Line 43: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:40:18 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 4_3: Line 44: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:41:33 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 12:41:33 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 12:41:33 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 12:41:33 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 12:41:33 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.3697647115476375, 'success rate': 1.0, 'levenshtein distance': 0.30371339700714456, 'similarity': None}\u001B[0m\n",
      "2024-07-20 12:41:33 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-124133.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [30:26<47:46, 204.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_pretr_T1_sc+_html+_single'...\n",
      "2024-07-20 12:41:33 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 12:41:40 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 10_3: Line 29: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:41:52 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 11_1: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:42:04 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 12_1: Line 24: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:42:57 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 16_3: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:43:44 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 25_1: Line 24: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:43:53 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 25_4: Line 29: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:44:08 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 26_4: Line 27: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:44:37 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 28_3: Line 46: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:44:43 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 2_1: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:45:00 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 3_1: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:46:17 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 9_3: Line 13: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:46:20 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 9_4: Line 50: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:46:28 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 12:46:28 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 12:46:28 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 12:46:28 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 12:46:28 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.39367352105727, 'success rate': 1.0, 'levenshtein distance': 0.2563518515503489, 'similarity': None}\u001B[0m\n",
      "2024-07-20 12:46:28 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-124628.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [35:22<50:20, 232.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_pretr_T1_sc+_html-_single'...\n",
      "2024-07-20 12:46:28 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 12:47:23 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 14_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:47:47 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 16_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:48:08 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 18_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:49:51 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 30_3: Line 47: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:51:26 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 12:51:26 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 12:51:26 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 12:51:26 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 12:51:26 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.28656779331403354, 'success rate': 1.0, 'levenshtein distance': 0.4101583529458054, 'similarity': None}\u001B[0m\n",
      "2024-07-20 12:51:26 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-125126.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [40:19<50:23, 251.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_pretr_T1_sc-_html+_single'...\n",
      "2024-07-20 12:51:26 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 12:51:42 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 10_6: Line 12: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:51:45 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 11_1: Line 24: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:51:59 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 12_2: Line 24: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:52:22 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 14_2: Line 36: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:52:34 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 15_3: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:52:57 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 17_3: Line 35: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:53:06 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 18_2: Line 33: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:53:24 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 1_1: Line 42: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:53:30 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 1_3: Line 25: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:53:38 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 25_2: Line 28: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:53:41 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 25_3: Line 34: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:53:44 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 25_4: Line 28: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:53:47 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 25_5: Line 29: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:53:50 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 26_1: Line 42: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:54:22 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 28_1: Line 42: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:54:34 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 2_1: Line 24: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:54:40 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 30_1: Line 24: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:54:52 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 3_1: Line 32: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:54:54 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 3_2: Line 27: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:55:03 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 4_3: Line 10: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:55:15 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 4_7: Line 7: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:55:18 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 5_1: Line 42: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:55:34 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 7_1: Line 32: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:55:46 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 8_1: Line 42: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:56:22 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 12:56:22 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 12:56:22 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 12:56:22 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 12:56:22 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.3315418635467669, 'success rate': 1.0, 'levenshtein distance': 0.33577284043554323, 'similarity': None}\u001B[0m\n",
      "2024-07-20 12:56:22 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-125622.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [45:15<48:37, 265.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_pretr_T1_sc-_html-_single'...\n",
      "2024-07-20 12:56:22 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 12:57:32 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 15_3: Line 27: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:59:12 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 27_2: Line 25: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 12:59:46 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 30_2: Line 26: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:00:01 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 4_1: Line 31: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:00:13 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 4_5: Line 44: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:00:42 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 7_3: Line 28: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:01:23 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 13:01:23 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 13:01:23 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 13:01:23 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 13:01:23 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.3065885980684633, 'success rate': 1.0, 'levenshtein distance': 0.38395617398417947, 'similarity': None}\u001B[0m\n",
      "2024-07-20 13:01:23 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-130123.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [50:17<46:02, 276.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_pretr_T2_sc+_html+_single'...\n",
      "2024-07-20 13:01:23 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 13:01:33 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 10_4: Line 29: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:02:47 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 16_3: Line 21: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:02:59 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 17_4: Line 20: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:04:06 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 27_1: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:04:29 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 28_4: Line 21: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:04:38 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 30_1: Line 25: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:04:41 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 30_2: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:05:32 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 7_1: Line 25: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:05:34 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 7_2: Line 34: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:06:11 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 9_4: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:06:20 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 13:06:20 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 13:06:20 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 13:06:20 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 13:06:20 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.3595209099195729, 'success rate': 1.0, 'levenshtein distance': 0.348674571339635, 'similarity': None}\u001B[0m\n",
      "2024-07-20 13:06:20 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-130620.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [55:13<42:21, 282.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_pretr_T3_sc+_html+_single'...\n",
      "2024-07-20 13:06:20 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 13:06:21 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 10_1: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:06:24 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 10_2: Line 32: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:06:31 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 10_4: Line 31: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:06:39 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 11_1: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:06:52 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 12_1: Line 24: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:06:58 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 12_3: Line 25: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:07:17 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 14_1: Line 35: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:07:20 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 14_2: Line 34: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:07:42 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 16_1: Line 35: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:07:46 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 16_2: Line 33: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:07:49 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 16_3: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:08:05 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 18_1: Line 35: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:08:08 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 18_2: Line 33: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:08:26 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 1_1: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:08:38 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 25_1: Line 24: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:08:41 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 25_2: Line 26: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:08:47 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 25_4: Line 29: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:08:50 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 25_5: Line 28: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:08:53 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 26_1: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:09:27 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 28_1: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:09:39 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 2_1: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:09:45 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 30_1: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:09:48 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 30_2: Line 31: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:10:04 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 4_1: Line 35: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:10:07 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 4_2: Line 32: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:10:10 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 4_3: Line 19: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:10:25 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 5_1: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:10:40 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 7_1: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:10:43 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 7_2: Line 32: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:10:52 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 8_1: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:11:10 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 9_1: Line 23: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:11:13 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 9_2: Line 32: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:11:16 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 9_3: Line 13: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:11:28 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 13:11:28 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 13:11:28 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 13:11:28 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 13:11:28 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.2659052214705403, 'success rate': 1.0, 'levenshtein distance': 0.39756964672408657, 'similarity': None}\u001B[0m\n",
      "2024-07-20 13:11:28 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-131128.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [1:00:21<38:39, 289.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_pretr_T4_sc+_html+_single'...\n",
      "2024-07-20 13:11:28 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 13:11:32 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 10_2: Line 32: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:11:46 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 11_1: Line 24: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:12:09 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 12_5: Line 24: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:12:48 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 16_2: Line 40: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:13:08 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 18_2: Line 38: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:14:38 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 2_1: Line 24: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:15:05 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 4_2: Line 32: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:15:41 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 7_2: Line 32: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:16:11 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 9_2: Line 32: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:16:26 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 13:16:26 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 13:16:26 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 13:16:26 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 13:16:26 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.35728405105267064, 'success rate': 1.0, 'levenshtein distance': 0.3434960266372642, 'similarity': None}\u001B[0m\n",
      "2024-07-20 13:16:26 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-131626.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [1:05:19<34:06, 292.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_pretr_T5_sc+_html+_all'...\n",
      "2024-07-20 13:16:26 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 13:16:32 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 10_3: Line 26: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:17:03 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 12_3: Line 33: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:17:21 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 14_1: Line 25: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:17:45 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 16_1: Line 25: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:18:06 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 18_1: Line 25: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:18:09 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 18_2: Line 41: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:18:45 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 25_3: Line 27: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:21:14 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 9_3: Line 28: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:21:25 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 13:21:25 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 13:21:25 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 13:21:25 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 13:21:25 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.4174046668014242, 'success rate': 1.0, 'levenshtein distance': 0.2562852252312224, 'similarity': None}\u001B[0m\n",
      "2024-07-20 13:21:25 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-132125.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19/25 [1:10:18<29:27, 294.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_pretr_T5_sc+_html+_single'...\n",
      "2024-07-20 13:21:25 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 13:23:43 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 25_4: Line 29: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:23:58 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 26_4: Line 27: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:24:41 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 30_2: Line 30: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:25:33 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 7_2: Line 28: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:26:17 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 13:26:17 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 13:26:17 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 13:26:17 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 13:26:17 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.4519280489127669, 'success rate': 1.0, 'levenshtein distance': 0.23181610064026517, 'similarity': None}\u001B[0m\n",
      "2024-07-20 13:26:17 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-132617.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [1:15:10<24:27, 293.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_pretr_T5_sc-_html+_single'...\n",
      "2024-07-20 13:26:17 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 13:26:56 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 12_4: Line 28: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:27:10 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 14_1: Line 35: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:27:31 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 15_5: Line 38: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:27:34 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 16_1: Line 35: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:27:54 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 18_1: Line 35: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:29:01 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 27_2: Line 34: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:29:42 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 3_1: Line 26: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:30:18 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 5_4: Line 37: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:31:11 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 13:31:11 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 13:31:11 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 13:31:11 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 13:31:11 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.4532069495505652, 'success rate': 1.0, 'levenshtein distance': 0.21960435749827611, 'similarity': None}\u001B[0m\n",
      "2024-07-20 13:31:11 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-133111.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [1:20:04<19:35, 293.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_template_1_html_concat_mode_single_max_attr_length_50_pretrained'...\n",
      "2024-07-20 13:31:11 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 13:36:10 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 13:36:10 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 13:36:10 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 13:36:10 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.38573672117195584, 'success rate': 1.0, 'levenshtein distance': 0.2914253065987123, 'similarity': None}\u001B[0m\n",
      "2024-07-20 13:36:10 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-133610.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [1:25:04<14:46, 295.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_template_1_no_html_pretrained'...\n",
      "2024-07-20 13:36:10 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 13:41:11 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 13:41:11 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 13:41:11 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 13:41:11 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.2354291135542711, 'success rate': 1.0, 'levenshtein distance': 0.46249194171936, 'similarity': None}\u001B[0m\n",
      "2024-07-20 13:41:11 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-134111.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [1:30:04<09:54, 297.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_template_2_html_concat_mode_all_max_attr_length_50_pretrained'...\n",
      "2024-07-20 13:41:11 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 13:44:38 [\u001B[34msrc.evaluation.metrics:132\u001B[0m] [\u001B[31mERROR\u001B[0m] >>>> Error calculating weighted bleu for test case 3_1: Line 19: Unexpected token ILLEGAL\u001B[0m\n",
      "2024-07-20 13:46:06 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 13:46:06 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 13:46:06 [\u001B[34msrc.evaluation.metrics:50\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Found None values in weighted bleu scores. Setting to 0...\u001B[0m\n",
      "2024-07-20 13:46:06 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 13:46:06 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.2522897304237842, 'success rate': 1.0, 'levenshtein distance': 0.4574200337451043, 'similarity': None}\u001B[0m\n",
      "2024-07-20 13:46:06 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-134606.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [1:34:59<04:56, 296.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Processing directory 'pred_test_script_template_2_html_concat_mode_single_max_attr_length_50_pretrained'...\n",
      "2024-07-20 13:46:06 [\u001B[34mscripts.evaluation:22\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Calculating scores...\u001B[0m\n",
      "2024-07-20 14:12:43 [\u001B[34mscripts.evaluation:46\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores calculated.\u001B[0m\n",
      "2024-07-20 14:12:43 [\u001B[34mscripts.evaluation:49\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Aggregating...\u001B[0m\n",
      "2024-07-20 14:12:43 [\u001B[34msrc.evaluation.metrics:57\u001B[0m] [\u001B[33mWARNING\u001B[0m] >>>> Unknown metric: similarity. Skipping...\u001B[0m\n",
      "2024-07-20 14:12:43 [\u001B[34mscripts.evaluation:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Scores aggregated: {'weighted bleu': 0.29558613115819576, 'success rate': 1.0, 'levenshtein distance': 0.4192614982918608, 'similarity': None}\u001B[0m\n",
      "2024-07-20 14:12:43 [\u001B[34mscripts.evaluation:66\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Results saved to ./data/scores/eval_scores_20240720-141243.pkl\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [2:01:37<00:00, 291.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 19s\n",
      "Wall time: 2h 1min 37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Go through prediction directory and run the renaming function and evaluation function on each directory\n",
    "results = []\n",
    "for dir in tqdm(os.listdir(\"data/prediction/\")):\n",
    "    if dir == '.gitkeep':\n",
    "        continue\n",
    "    print('\\n'+'='*80+'\\n')\n",
    "    print(f\"Processing directory '{dir}'...\")\n",
    "    rename_files_in_directory(\"data/prediction/\" + dir)\n",
    "    config['paths']['prediction_dir'] = \"data/prediction/\" + dir + \"/\"\n",
    "    result = evaluate_test_cases(config)\n",
    "    results.append(result)\n",
    "\n",
    "# merge\n",
    "results = pd.concat(results).reset_index(drop=True)\n",
    "results.to_pickle(config['paths']['scores_dir']+'eval_scores_all.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T12:12:43.939183500Z",
     "start_time": "2024-07-20T10:11:06.647431800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "     file_id test_case test_step  \\\n0        1_2         1         2   \n1        2_2         2         2   \n2       10_1        10         1   \n3       10_2        10         2   \n4       10_3        10         3   \n...      ...       ...       ...   \n1981     9_2         9         2   \n1982     9_3         9         3   \n1983     9_4         9         4   \n1984     9_5         9         5   \n1985     9_6         9         6   \n\n                                         prediction_dir  weighted bleu  \\\n0                     data/prediction/pred_test_script/       0.500697   \n1                     data/prediction/pred_test_script/       0.398845   \n2     data/prediction/pred_test_script_finetuned_T1_...       0.513906   \n3     data/prediction/pred_test_script_finetuned_T1_...       0.725558   \n4     data/prediction/pred_test_script_finetuned_T1_...       0.900160   \n...                                                 ...            ...   \n1981  data/prediction/pred_test_script_template_2_ht...       0.563547   \n1982  data/prediction/pred_test_script_template_2_ht...       0.000000   \n1983  data/prediction/pred_test_script_template_2_ht...       0.000000   \n1984  data/prediction/pred_test_script_template_2_ht...       0.340417   \n1985  data/prediction/pred_test_script_template_2_ht...       0.433774   \n\n      success rate  levenshtein distance  similarity  \n0                1              0.092308           0  \n1                1              0.263240           0  \n2                1              0.578158           0  \n3                1              0.029940           0  \n4                1              0.013831           0  \n...            ...                   ...         ...  \n1981             1              0.224000           0  \n1982             1              0.816187           0  \n1983             1              0.831671           0  \n1984             1              0.259560           0  \n1985             1              0.177489           0  \n\n[1986 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_id</th>\n      <th>test_case</th>\n      <th>test_step</th>\n      <th>prediction_dir</th>\n      <th>weighted bleu</th>\n      <th>success rate</th>\n      <th>levenshtein distance</th>\n      <th>similarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1_2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>data/prediction/pred_test_script/</td>\n      <td>0.500697</td>\n      <td>1</td>\n      <td>0.092308</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2_2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>data/prediction/pred_test_script/</td>\n      <td>0.398845</td>\n      <td>1</td>\n      <td>0.263240</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10_1</td>\n      <td>10</td>\n      <td>1</td>\n      <td>data/prediction/pred_test_script_finetuned_T1_...</td>\n      <td>0.513906</td>\n      <td>1</td>\n      <td>0.578158</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10_2</td>\n      <td>10</td>\n      <td>2</td>\n      <td>data/prediction/pred_test_script_finetuned_T1_...</td>\n      <td>0.725558</td>\n      <td>1</td>\n      <td>0.029940</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10_3</td>\n      <td>10</td>\n      <td>3</td>\n      <td>data/prediction/pred_test_script_finetuned_T1_...</td>\n      <td>0.900160</td>\n      <td>1</td>\n      <td>0.013831</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1981</th>\n      <td>9_2</td>\n      <td>9</td>\n      <td>2</td>\n      <td>data/prediction/pred_test_script_template_2_ht...</td>\n      <td>0.563547</td>\n      <td>1</td>\n      <td>0.224000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1982</th>\n      <td>9_3</td>\n      <td>9</td>\n      <td>3</td>\n      <td>data/prediction/pred_test_script_template_2_ht...</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>0.816187</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1983</th>\n      <td>9_4</td>\n      <td>9</td>\n      <td>4</td>\n      <td>data/prediction/pred_test_script_template_2_ht...</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>0.831671</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1984</th>\n      <td>9_5</td>\n      <td>9</td>\n      <td>5</td>\n      <td>data/prediction/pred_test_script_template_2_ht...</td>\n      <td>0.340417</td>\n      <td>1</td>\n      <td>0.259560</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1985</th>\n      <td>9_6</td>\n      <td>9</td>\n      <td>6</td>\n      <td>data/prediction/pred_test_script_template_2_ht...</td>\n      <td>0.433774</td>\n      <td>1</td>\n      <td>0.177489</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1986 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load saved evaluation results of different predictions\n",
    "results = pd.read_pickle(config['paths']['scores_dir']+'eval_scores_all.pkl')\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T12:13:17.967569100Z",
     "start_time": "2024-07-20T12:13:17.863376400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1986 entries, 0 to 1985\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   file_id               1986 non-null   object \n",
      " 1   test_case             1986 non-null   object \n",
      " 2   test_step             1986 non-null   object \n",
      " 3   prediction_dir        1986 non-null   object \n",
      " 4   weighted bleu         1799 non-null   float64\n",
      " 5   success rate          1986 non-null   int64  \n",
      " 6   levenshtein distance  1986 non-null   float64\n",
      " 7   similarity            1986 non-null   int64  \n",
      "dtypes: float64(2), int64(2), object(4)\n",
      "memory usage: 124.3+ KB\n"
     ]
    }
   ],
   "source": [
    "results.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T12:13:07.191906500Z",
     "start_time": "2024-07-20T12:13:07.076953300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display evaluation results in a boxplot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.boxplot(data=results, ax=ax, orient='h')\n",
    "ax.set_title('Evaluation Results', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-19T10:28:15.553544500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "[1]\n",
    "```bibtex\n",
    "@INPROCEEDINGS{Papineni02bleu:a,\n",
    "    author = {Kishore Papineni and Salim Roukos and Todd Ward and Wei-jing Zhu},\n",
    "    title = {BLEU: a Method for Automatic Evaluation of Machine Translation},\n",
    "    booktitle = {},\n",
    "    year = {2002},\n",
    "    pages = {311--318}\n",
    "}\n",
    "```\n",
    "[2]\n",
    "```bibtex\n",
    "@inproceedings{lin-och-2004-orange,\n",
    "    title = \"{ORANGE}: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation\",\n",
    "    author = \"Lin, Chin-Yew  and\n",
    "      Och, Franz Josef\",\n",
    "    booktitle = \"{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics\",\n",
    "    month = \"aug 23{--}aug 27\",\n",
    "    year = \"2004\",\n",
    "    address = \"Geneva, Switzerland\",\n",
    "    publisher = \"COLING\",\n",
    "    url = \"https://www.aclweb.org/anthology/C04-1072\",\n",
    "    pages = \"501--507\",\n",
    "}\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
