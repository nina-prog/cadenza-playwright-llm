{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T04:10:14.281826100Z",
     "start_time": "2024-07-22T04:10:14.239095800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.evaluation.metrics import aggregate_scores, calculate_scores\n",
    "from src.data.data_loading import load_config\n",
    "from src.data.code_processor import parse_code"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T04:10:21.025391500Z",
     "start_time": "2024-07-22T04:10:14.281826100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "plt.rcParams['font.family'] = 'STIXGeneral'\n",
    "plt.rcParams['font.size'] = 12\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "# Set color palette"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T04:10:21.124639900Z",
     "start_time": "2024-07-22T04:10:21.025391500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set working directory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\merti\\\\PycharmProjects\\\\cadenza-playwright-llm'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set working directory to project root - EXECUTE ONLY ONCE or RESTART KERNEL\n",
    "os.chdir('../..')\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T04:10:41.365248500Z",
     "start_time": "2024-07-22T04:10:41.247447300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load configuration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "config = load_config(config_path='config/config.yaml')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T04:10:51.514473500Z",
     "start_time": "2024-07-22T04:10:51.390689900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Methods Summary\n",
    "\n",
    "Metrics implemented and used in this project (see `src/evaluation/metrics.py`) in the evaluation process:\n",
    "\n",
    "### 1. **Weighted BLEU Score** $ \\in [0.0, 1.0] $\n",
    "- **Purpose**: The BLEU score proposed by [Papineni et al. (2002)](https://aclanthology.org/P02-1040.pdf) [1], [2] is a metric that measures the similarity between two sequences of text. The weighted BLEU score is a variant implementd in this project that uses a weighted average of the BLEU scores of the precondition part and the actual generated additional part in teh generated test script. Measures the quality of the generated code by comparing it to reference (validation) code.\n",
    "- **Description**: This method calculates the BLEU score with two components:\n",
    "  - **Precondition Code Accuracy**: Evaluates how well the generated code matches the precondition code.\n",
    "  - **New Lines Accuracy**: Assesses how well the generated code meets the goals by comparing the new lines added.\n",
    "- **Formula**: \n",
    "  \n",
    "  Weighted BLEU Score = (1 - α) * First BLEU Score + α * Second BLEU Score\n",
    "  \n",
    "  Where \\(\\alpha\\) is a weight factor, typically set to 0.5.\n",
    "- **Output**: A floating-point score indicating the degree of similarity between the generated and validation code.\n",
    "\n",
    "### 2. **Success Rate** $ \\in [0.0, 1.0] $\n",
    "- **Purpose**: Evaluates whether the generated code successfully performs the intended functionality.\n",
    "- **Description**: The generated code is executed in a testing environment, and the success rate is determined based on the test's result.\n",
    "- **Procedure**:\n",
    "  1. Modify the generated code to include screenshot commands.\n",
    "  2. Save the updated code to a temporary file.\n",
    "  3. Run the Playwright test.\n",
    "  4. Return `1` if the test passes, otherwise `0`.\n",
    "- **Output**: A binary score (`1` or `0`) representing test success.\n",
    "\n",
    "### 3. **Levenshtein Distance** $ d(s, t) \\in \\mathbb{N} $\n",
    "- **Purpose**: Measures the similarity between the generated code and validation code based on edit distance.\n",
    "- **Description**: The Levenshtein distance between strings $ s $ and $ t $ is an integer that measures the number of single-character edits (insertions, deletions, or substitutions) needed to change the generated code into the validation code. The distance is normalized by the length of the longer code.\n",
    "- **Formula**:\n",
    "  Levenshtein Distance = Edit Distance / Max Length of Generated and Validation Code\n",
    "- **Output**: A floating-point score between 0 and 1, where lower values indicate higher similarity.\n",
    "\n",
    "### 4. **Similarity (Cosine Similarity)**\n",
    "- **Purpose**: Assesses the similarity between screenshots of the generated code and the ground truth.\n",
    "- **Description**: Uses a pre-trained ResNet-18 model to extract image embeddings and calculates the cosine similarity between embeddings of the predicted and ground truth images.\n",
    "- **Procedure**:\n",
    "  1. Load and preprocess the screenshots.\n",
    "  2. Compute embeddings using the ResNet-18 model.\n",
    "  3. Calculate cosine similarity between the embeddings.\n",
    "- **Output**: A floating-point score between 0 and 1, indicating the similarity between the images.\n",
    "\n",
    "## Evaluation Storage\n",
    "\n",
    "- **Evaluation Data**: Evaluation results for different templates and options can be found under `data/scores`.\n",
    "- **Evaluation Code**: The code used for evaluation is stored in `src/eval/metrics.py`.\n",
    "\n",
    "These metrics collectively provide a comprehensive assessment of the generated code's quality and effectiveness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Summary\n",
    "\n",
    "The evaluation of generated code was conducted across various templates and configurations, addressing different aspects such as image presence, HTML inclusion, and model fine-tuning. The following types of evaluations were performed:\n",
    "\n",
    "### 1. **Templates with Images**\n",
    "- **Examples**: `pred_test_script_pretr_T4_sc+_html+_single`, `pred_test_script_finetuned_T5_sc+_html+_single`\n",
    "- **Description**: Evaluated scripts that included image-based components, comparing generated outputs with expected results for scenarios where images were part of the test.\n",
    "\n",
    "### 2. **Templates without Images**\n",
    "- **Examples**: `pred_test_script_template_1_no_html_pretrained`, `pred_test_script_pretr_T1_sc-_html-_single`\n",
    "- **Description**: Focused on templates where no images were included. This evaluated the performance and accuracy of generated code in the absence of image-based validation.\n",
    "\n",
    "### 3. **HTML vs. No HTML**\n",
    "- **Examples**: `pred_test_script_pretr_T1_sc+_html-_single`, `pred_test_script_finetuned_T5_sc-_html+_single`\n",
    "- **Description**: Assessed how well the generated scripts handled scenarios with and without HTML components, testing their effectiveness in different contexts.\n",
    "\n",
    "### 4. **Single vs. All Configurations**\n",
    "- **Examples**: `pred_test_script_pretr_T5_sc+_html+_single`, `pred_test_script_pretr_T5_sc+_html+_all`\n",
    "- **Description**: Included evaluations for both single-instance and all-instance configurations to determine the performance across various levels of complexity and data variety.\n",
    "\n",
    "### 5. **Pretrained vs. Finetuned Models**\n",
    "- **Examples**: `pred_test_script_finetuned_T5_sc+_html+_single`, `pred_test_script_pretr_T1_sc-_html+_single`\n",
    "- **Description**: Compared results from pretrained models against those from finetuned models to assess improvements and differences in performance and accuracy.\n",
    "\n",
    "### 6. **Different Attribute Lengths and Concatenation Modes**\n",
    "- **Examples**: `pred_test_script_template_2_html_concat_mode_single_max_attr_length_50_pretrained`, `pred_test_script_template_2_html_concat_mode_all_max_attr_length_50_pretrained`\n",
    "- **Description**: Evaluated the impact of different attribute lengths and concatenation modes on the performance of generated code.\n",
    "\n",
    "\n",
    "This evaluation approach ensured a comprehensive assessment of the generated code under multiple conditions and configurations, providing insights into the effectiveness and accuracy of the different methods.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Weighted BLEU Score  \n",
    "\n",
    "For the scoring of our generated predictions, we chose multiple scores. Our foremost score is the BLEU score, which is a frequent metric for LLMs, so we chose to implement it here as well.\n",
    "The BLEU score uses a similarity measure between the n-grams of the sample compared to the references he gets.\n",
    "The reference in our case is the human-made code for the described test step. We chose to use the default configuration for the BLEU weights, which utilizes 1-grams up to 4-grams. All n-grams are uniformly weighted and have equal weight in the result."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "See [src.evaluation.metrics](../) for our implementation of the BLEU score."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Our weighted Bleu score separately evaluates the code from the precondition, which is copied by the LLM, and the newly generated code for the current step. This is intended to decouple the final result from the length of the precondition. Both parts are evaluated with 50 percent each."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Levenshtein Distance\n",
    "The scoring function for our validation samples is the Levenshtein distance. This measures the distance between two strings by the amount of necessary single-character operations to turn one string into another. These operations are remove, add, and replace. So the maximum distance this measure can calculate is the maximum length of input strings. Since later tests involve longer preconditions and descriptions the chance for mistakes is higher, so the Levenshtein distance would always increase for the longer test and be lower for the shorter tests. To counteract this, we normalize the distance to values between 0 and 1 by dividing with the maximum length of the input strings. This represents small errors or deviations in tests much better since those should perform better than very short and error-riddled tests."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "See [src.evaluation.metrics](../) for our implementation of the Levenshtein distance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluation Summary"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T04:27:52.401647400Z",
     "start_time": "2024-07-22T04:27:52.313979200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_pickle(config['paths']['scores_dir']+'eval_scores_all_20240721.pkl')"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T04:28:12.437058100Z",
     "start_time": "2024-07-22T04:28:12.342257800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# exclude all but the relevant directories\n",
    "list = df['prediction_dir'].unique()\n",
    "excluded_list = []\n",
    "for i in list:\n",
    "    if \"max_attr\" in i:\n",
    "        excluded_list.append(i)\n",
    "    elif \"test_set\" in i:\n",
    "        excluded_list.append(i)\n",
    "    elif \"_sc\" not in i:\n",
    "        excluded_list.append(i)\n",
    "    elif \"_html\" not in i:\n",
    "        excluded_list.append(i)\n",
    "    elif \"_T\" not in i:\n",
    "        excluded_list.append(i)\n",
    "rel_list = [x for x in list if x not in excluded_list]"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   weighted bleu          \\\n                                                            mean     std   \nprediction_dir                                                             \ndata/prediction/pred_test_script_finetuned_T1_s...        0.5993  0.1202   \ndata/prediction/pred_test_script_finetuned_T1_s...        0.7882  0.1359   \ndata/prediction/pred_test_script_finetuned_T5_s...        0.6330  0.1097   \ndata/prediction/pred_test_script_finetuned_T5_s...        0.7871  0.1242   \ndata/prediction/pred_test_script_pretr_T1_sc+_h...        0.3976  0.1302   \ndata/prediction/pred_test_script_pretr_T1_sc+_h...        0.4324  0.1118   \ndata/prediction/pred_test_script_pretr_T1_sc+_h...        0.2991  0.1503   \ndata/prediction/pred_test_script_pretr_T1_sc-_h...        0.4189  0.1774   \ndata/prediction/pred_test_script_pretr_T1_sc-_h...        0.3211  0.2021   \ndata/prediction/pred_test_script_pretr_T2_sc+_h...        0.3987  0.1322   \ndata/prediction/pred_test_script_pretr_T3_sc+_h...        0.3551  0.1319   \ndata/prediction/pred_test_script_pretr_T4_sc+_h...        0.3796  0.1168   \ndata/prediction/pred_test_script_pretr_T5_sc+_h...        0.4483  0.1124   \ndata/prediction/pred_test_script_pretr_T5_sc+_h...        0.4694  0.0743   \ndata/prediction/pred_test_script_pretr_T5_sc-_h...        0.4943  0.2009   \n\n                                                   levenshtein distance  \\\n                                                                   mean   \nprediction_dir                                                            \ndata/prediction/pred_test_script_finetuned_T1_s...               0.3145   \ndata/prediction/pred_test_script_finetuned_T1_s...               0.0726   \ndata/prediction/pred_test_script_finetuned_T5_s...               0.2088   \ndata/prediction/pred_test_script_finetuned_T5_s...               0.0629   \ndata/prediction/pred_test_script_pretr_T1_sc+_h...               0.3037   \ndata/prediction/pred_test_script_pretr_T1_sc+_h...               0.2564   \ndata/prediction/pred_test_script_pretr_T1_sc+_h...               0.4102   \ndata/prediction/pred_test_script_pretr_T1_sc-_h...               0.3358   \ndata/prediction/pred_test_script_pretr_T1_sc-_h...               0.3840   \ndata/prediction/pred_test_script_pretr_T2_sc+_h...               0.3487   \ndata/prediction/pred_test_script_pretr_T3_sc+_h...               0.3976   \ndata/prediction/pred_test_script_pretr_T4_sc+_h...               0.3435   \ndata/prediction/pred_test_script_pretr_T5_sc+_h...               0.2563   \ndata/prediction/pred_test_script_pretr_T5_sc+_h...               0.2318   \ndata/prediction/pred_test_script_pretr_T5_sc-_h...               0.2196   \n\n                                                           similarity       \\\n                                                       std       mean  std   \nprediction_dir                                                               \ndata/prediction/pred_test_script_finetuned_T1_s...  0.2346        0.0  0.0   \ndata/prediction/pred_test_script_finetuned_T1_s...  0.0998        0.0  0.0   \ndata/prediction/pred_test_script_finetuned_T5_s...  0.2115        0.0  0.0   \ndata/prediction/pred_test_script_finetuned_T5_s...  0.0872        0.0  0.0   \ndata/prediction/pred_test_script_pretr_T1_sc+_h...  0.2029        0.0  0.0   \ndata/prediction/pred_test_script_pretr_T1_sc+_h...  0.1863        0.0  0.0   \ndata/prediction/pred_test_script_pretr_T1_sc+_h...  0.2051        0.0  0.0   \ndata/prediction/pred_test_script_pretr_T1_sc-_h...  0.2229        0.0  0.0   \ndata/prediction/pred_test_script_pretr_T1_sc-_h...  0.2006        0.0  0.0   \ndata/prediction/pred_test_script_pretr_T2_sc+_h...  0.1987        0.0  0.0   \ndata/prediction/pred_test_script_pretr_T3_sc+_h...  0.2565        0.0  0.0   \ndata/prediction/pred_test_script_pretr_T4_sc+_h...  0.2236        0.0  0.0   \ndata/prediction/pred_test_script_pretr_T5_sc+_h...  0.1739        0.0  0.0   \ndata/prediction/pred_test_script_pretr_T5_sc+_h...  0.1320        0.0  0.0   \ndata/prediction/pred_test_script_pretr_T5_sc-_h...  0.2048        0.0  0.0   \n\n                                                   success rate       \n                                                           mean  std  \nprediction_dir                                                        \ndata/prediction/pred_test_script_finetuned_T1_s...         0.01  0.1  \ndata/prediction/pred_test_script_finetuned_T1_s...         0.01  0.1  \ndata/prediction/pred_test_script_finetuned_T5_s...         0.01  0.1  \ndata/prediction/pred_test_script_finetuned_T5_s...         0.01  0.1  \ndata/prediction/pred_test_script_pretr_T1_sc+_h...         0.01  0.1  \ndata/prediction/pred_test_script_pretr_T1_sc+_h...         0.01  0.1  \ndata/prediction/pred_test_script_pretr_T1_sc+_h...         0.01  0.1  \ndata/prediction/pred_test_script_pretr_T1_sc-_h...         0.01  0.1  \ndata/prediction/pred_test_script_pretr_T1_sc-_h...         0.01  0.1  \ndata/prediction/pred_test_script_pretr_T2_sc+_h...         0.01  0.1  \ndata/prediction/pred_test_script_pretr_T3_sc+_h...         0.01  0.1  \ndata/prediction/pred_test_script_pretr_T4_sc+_h...         0.01  0.1  \ndata/prediction/pred_test_script_pretr_T5_sc+_h...         0.01  0.1  \ndata/prediction/pred_test_script_pretr_T5_sc+_h...         0.01  0.1  \ndata/prediction/pred_test_script_pretr_T5_sc-_h...         0.01  0.1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"2\" halign=\"left\">weighted bleu</th>\n      <th colspan=\"2\" halign=\"left\">levenshtein distance</th>\n      <th colspan=\"2\" halign=\"left\">similarity</th>\n      <th colspan=\"2\" halign=\"left\">success rate</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>mean</th>\n      <th>std</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>mean</th>\n      <th>std</th>\n    </tr>\n    <tr>\n      <th>prediction_dir</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>data/prediction/pred_test_script_finetuned_T1_sc+_html+_single/</th>\n      <td>0.5993</td>\n      <td>0.1202</td>\n      <td>0.3145</td>\n      <td>0.2346</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>data/prediction/pred_test_script_finetuned_T1_sc-_html+_single/</th>\n      <td>0.7882</td>\n      <td>0.1359</td>\n      <td>0.0726</td>\n      <td>0.0998</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>data/prediction/pred_test_script_finetuned_T5_sc+_html+_single/</th>\n      <td>0.6330</td>\n      <td>0.1097</td>\n      <td>0.2088</td>\n      <td>0.2115</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>data/prediction/pred_test_script_finetuned_T5_sc-_html+_single/</th>\n      <td>0.7871</td>\n      <td>0.1242</td>\n      <td>0.0629</td>\n      <td>0.0872</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>data/prediction/pred_test_script_pretr_T1_sc+_html+_all/</th>\n      <td>0.3976</td>\n      <td>0.1302</td>\n      <td>0.3037</td>\n      <td>0.2029</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>data/prediction/pred_test_script_pretr_T1_sc+_html+_single/</th>\n      <td>0.4324</td>\n      <td>0.1118</td>\n      <td>0.2564</td>\n      <td>0.1863</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>data/prediction/pred_test_script_pretr_T1_sc+_html-_single/</th>\n      <td>0.2991</td>\n      <td>0.1503</td>\n      <td>0.4102</td>\n      <td>0.2051</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>data/prediction/pred_test_script_pretr_T1_sc-_html+_single/</th>\n      <td>0.4189</td>\n      <td>0.1774</td>\n      <td>0.3358</td>\n      <td>0.2229</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>data/prediction/pred_test_script_pretr_T1_sc-_html-_single/</th>\n      <td>0.3211</td>\n      <td>0.2021</td>\n      <td>0.3840</td>\n      <td>0.2006</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>data/prediction/pred_test_script_pretr_T2_sc+_html+_single/</th>\n      <td>0.3987</td>\n      <td>0.1322</td>\n      <td>0.3487</td>\n      <td>0.1987</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>data/prediction/pred_test_script_pretr_T3_sc+_html+_single/</th>\n      <td>0.3551</td>\n      <td>0.1319</td>\n      <td>0.3976</td>\n      <td>0.2565</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>data/prediction/pred_test_script_pretr_T4_sc+_html+_single/</th>\n      <td>0.3796</td>\n      <td>0.1168</td>\n      <td>0.3435</td>\n      <td>0.2236</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>data/prediction/pred_test_script_pretr_T5_sc+_html+_all/</th>\n      <td>0.4483</td>\n      <td>0.1124</td>\n      <td>0.2563</td>\n      <td>0.1739</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>data/prediction/pred_test_script_pretr_T5_sc+_html+_single/</th>\n      <td>0.4694</td>\n      <td>0.0743</td>\n      <td>0.2318</td>\n      <td>0.1320</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>data/prediction/pred_test_script_pretr_T5_sc-_html+_single/</th>\n      <td>0.4943</td>\n      <td>0.2009</td>\n      <td>0.2196</td>\n      <td>0.2048</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.01</td>\n      <td>0.1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate the results for each prediction directory\n",
    "g = df[df['prediction_dir'].isin(rel_list)]\n",
    "results = g.groupby('prediction_dir').agg({'weighted bleu': ['mean', 'std'],\n",
    "                                           'levenshtein distance': ['mean', 'std'],\n",
    "                                           'similarity': ['mean', 'std'],\n",
    "                                           'success rate': ['mean', 'std']})\n",
    "# strip to 2 decimal places\n",
    "results = results.round(4)\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T05:03:42.051591400Z",
     "start_time": "2024-07-22T05:03:41.915757400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Mean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template 1: \n",
      " BLEU: 0.4324\n",
      " Levenshtein: 0.2564\n",
      " Similarity: 0.0\n",
      " Success Rate: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Baseline pretrained_model, Template1 with screenshots with html concat-mode: single\n",
    "base = 'data/prediction/pred_test_script_pretr_T1_sc+_html+_single/'\n",
    "print(\"Template 1: \\n\", f\"BLEU: {results.loc[base]['weighted bleu']['mean']}\\n\", f\"Levenshtein: {results.loc[base]['levenshtein distance']['mean']}\\n\", f\"Similarity: {results.loc[base]['similarity']['mean']}\\n\", f\"Success Rate: {results.loc[base]['success rate']['mean']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T04:46:18.856335700Z",
     "start_time": "2024-07-22T04:46:18.746052300Z"
    }
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is our baseline performance for this evaluation. The template 1 with screenshots and html concat-mode single.\n",
    "* Success rate is so low due to many tiny errors in the generated code. This can be just a missing space or a missing bracket which leads to a failing test. But also a duplicated line or a missing line can lead to a failing test. Therefore for evaluation the success rate as well as the similarity which is dependent on the success rate. Simliarity is calculated by comparing the screenshots of the generated code and the human-made code. If the test fails, no screenshot is taken of the actual outcome of the website action and therefore the similarity is 0."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Template variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "score_dic = {}\n",
    "for i in rel_list:\n",
    "    score_dic[i] = f\"BLEU: {results.loc[i]['weighted bleu']['mean']}, Levenshtein: {results.loc[i]['levenshtein distance']['mean']}, Similarity: {results.loc[i]['similarity']['mean']}, Success Rate: {results.loc[i]['success rate']['mean']}\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T04:51:24.145044400Z",
     "start_time": "2024-07-22T04:51:24.044200500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template 1:  BLEU: 0.4324, Levenshtein: 0.2564, Similarity: 0.0, Success Rate: 0.01\n",
      "Template 2:  BLEU: 0.3987, Levenshtein: 0.3487, Similarity: 0.0, Success Rate: 0.01\n",
      "Template 3:  BLEU: 0.3551, Levenshtein: 0.3976, Similarity: 0.0, Success Rate: 0.01\n",
      "Template 4:  BLEU: 0.3796, Levenshtein: 0.3435, Similarity: 0.0, Success Rate: 0.01\n",
      "Template 5:  BLEU: 0.4694, Levenshtein: 0.2318, Similarity: 0.0, Success Rate: 0.01\n"
     ]
    }
   ],
   "source": [
    "temp2 = 'data/prediction/pred_test_script_pretr_T2_sc+_html+_single/'\n",
    "temp3 = 'data/prediction/pred_test_script_pretr_T3_sc+_html+_single/'\n",
    "temp4 = 'data/prediction/pred_test_script_pretr_T4_sc+_html+_single/'\n",
    "temp5 = 'data/prediction/pred_test_script_pretr_T5_sc+_html+_single/'\n",
    "\n",
    "print(\"Template 1: \", score_dic[base])\n",
    "print(\"Template 2: \", score_dic[temp2])\n",
    "print(\"Template 3: \", score_dic[temp3])\n",
    "print(\"Template 4: \", score_dic[temp4])\n",
    "print(\"Template 5: \", score_dic[temp5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T04:51:27.066565100Z",
     "start_time": "2024-07-22T04:51:26.969456600Z"
    }
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we see above, the base case with template 1 and template 5 perform far better than the other templates.\n",
    "Though it is to be mentioned that there is also large gap between template 1 und 5 in favor of template 5"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Variations with contextual informations"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T04:51:51.460358Z",
     "start_time": "2024-07-22T04:51:51.374223400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "no_scr = 'data/prediction/pred_test_script_pretr_T1_sc-_html+_single/'\n",
    "no_html = 'data/prediction/pred_test_script_pretr_T1_sc+_html-_single/'\n",
    "no_scrNhtml = 'data/prediction/pred_test_script_pretr_T1_sc-_html-_single/'\n",
    "\n",
    "print(\"Template 1: \", score_dic[base])\n",
    "print(\"No Screenshot: \", score_dic[no_scr])\n",
    "print(\"No HTML: \", score_dic[no_html])\n",
    "print(\"Neither: \", score_dic[no_scrNhtml])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template 1:  BLEU: 0.4324, Levenshtein: 0.2564, Similarity: 0.0, Success Rate: 0.01\n",
      "No Screenshot:  BLEU: 0.4189, Levenshtein: 0.3358, Similarity: 0.0, Success Rate: 0.01\n",
      "No HTML:  BLEU: 0.2991, Levenshtein: 0.4102, Similarity: 0.0, Success Rate: 0.01\n",
      "Neither:  BLEU: 0.3211, Levenshtein: 0.384, Similarity: 0.0, Success Rate: 0.01\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As expected the base case with the most contextual information performs the best. But important to notice the case without the screenshot is not far behind. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Variations within processing parameters"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T04:51:59.607410600Z",
     "start_time": "2024-07-22T04:51:59.505289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "T1_concat_all = 'data/prediction/pred_test_script_pretr_T1_sc+_html+_all/'\n",
    "T5_concat_single = 'data/prediction/pred_test_script_pretr_T5_sc+_html+_single/'\n",
    "T5_concat_all = 'data/prediction/pred_test_script_pretr_T5_sc+_html+_all/'\n",
    "print(\"T1 - Base/Single: \", score_dic[base])\n",
    "print(\"T1 - All: \", score_dic[T1_concat_all])\n",
    "\n",
    "print(\"T5 - Single: \", score_dic[T5_concat_single])\n",
    "print(\"T5 - All: \", score_dic[T5_concat_all])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 - Base/Single:  BLEU: 0.4324, Levenshtein: 0.2564, Similarity: 0.0, Success Rate: 0.01\n",
      "T1 - All:  BLEU: 0.3976, Levenshtein: 0.3037, Similarity: 0.0, Success Rate: 0.01\n",
      "T5 - Single:  BLEU: 0.4694, Levenshtein: 0.2318, Similarity: 0.0, Success Rate: 0.01\n",
      "T5 - All:  BLEU: 0.4483, Levenshtein: 0.2563, Similarity: 0.0, Success Rate: 0.01\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Also the \"single\"-concat mode seems to outperform \"all\"-concat mode for both our top performing templates."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Pretrained vs Finetuned"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T04:52:07.046315400Z",
     "start_time": "2024-07-22T04:52:06.953422600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base = 'data/prediction/pred_test_script_pretr_T1_sc+_html+_single/'\n",
    "pre_t5 = 'data/prediction/pred_test_script_pretr_T5_sc+_html+_single/'\n",
    "\n",
    "print(\"Pre - Template 1: \", score_dic[base])\n",
    "print(\"Pre - Template 5: \", score_dic[pre_t5])\n",
    "\n",
    "ft_t1 = 'data/prediction/pred_test_script_finetuned_T1_sc+_html+_single/'\n",
    "ft_t5  = 'data/prediction/pred_test_script_finetuned_T5_sc+_html+_single/'\n",
    "\n",
    "print(\"Ft - Template 1: \", score_dic[ft_t1])\n",
    "print(\"Ft - Template 5: \", score_dic[ft_t5])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre - Template 1:  BLEU: 0.4324, Levenshtein: 0.2564, Similarity: 0.0, Success Rate: 0.01\n",
      "Pre - Template 5:  BLEU: 0.4694, Levenshtein: 0.2318, Similarity: 0.0, Success Rate: 0.01\n",
      "Ft - Template 1:  BLEU: 0.5993, Levenshtein: 0.3145, Similarity: 0.0, Success Rate: 0.01\n",
      "Ft - Template 5:  BLEU: 0.633, Levenshtein: 0.2088, Similarity: 0.0, Success Rate: 0.01\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Our finetuning did improve our performance quite a bit. Both templates reach a BLEU score above 50 which means they are able to accurately return the precondition and also manage to generate somewhat fitting new code."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><img src=\"./finetunevspretrained.png\" width=\"1000\" ><br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finetuning with or without screenshots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T04:52:11.551264300Z",
     "start_time": "2024-07-22T04:52:11.455833700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "t1_w = 'data/prediction/pred_test_script_finetuned_T1_sc+_html+_single/'\n",
    "t5_w  = 'data/prediction/pred_test_script_finetuned_T5_sc+_html+_single/'\n",
    "\n",
    "t1_wo = 'data/prediction/pred_test_script_finetuned_T1_sc-_html+_single/'\n",
    "t5_wo = 'data/prediction/pred_test_script_finetuned_T5_sc-_html+_single/'\n",
    "\n",
    "print(\"With Screenshot - Template 1: \", score_dic[t1_w])\n",
    "print(\"With Screenshot - Template 5: \", score_dic[t5_w])\n",
    "\n",
    "print(\"Without Screenshot - Template 1: \", score_dic[t1_wo])\n",
    "print(\"Without Screenshot - Template 5: \", score_dic[t5_wo])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Screenshot - Template 1:  BLEU: 0.5993, Levenshtein: 0.3145, Similarity: 0.0, Success Rate: 0.01\n",
      "With Screenshot - Template 5:  BLEU: 0.633, Levenshtein: 0.2088, Similarity: 0.0, Success Rate: 0.01\n",
      "Without Screenshot - Template 1:  BLEU: 0.7882, Levenshtein: 0.0726, Similarity: 0.0, Success Rate: 0.01\n",
      "Without Screenshot - Template 5:  BLEU: 0.7871, Levenshtein: 0.0629, Similarity: 0.0, Success Rate: 0.01\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "The version of the finetuned model which used the templates without screenshots seemed to perform the best by far. Independent of the template.<br>\n",
    "Which seems to indicate that the most valuable data can be gained without screenshot and can be easier understood if the noise generated by the screenshot is removed.\n",
    "\n",
    "Thus, in summary the finetuned models without screenshots perform by far the best. Though if we were to only look at the BLEU Score, template 1 seems to be slightly favored.<br>\n",
    "But in regard to how close we got to the human-made test script, template 5 seems to be slightly ahead as indicated by the Levenshtein distance.\n",
    "So we can't identify a clear winner, template 5 seems to perform on average the best over all observed configurations.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
