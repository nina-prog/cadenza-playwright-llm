{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Methods Summary\n",
    "\n",
    "In the evaluation process, several metrics are used to compare generated code with validation code. The metrics include:\n",
    "\n",
    "### 1. **Weighted BLEU Score**\n",
    "- **Purpose**: Measures the quality of the generated code by comparing it to reference (validation) code.\n",
    "- **Description**: This method calculates the BLEU score with two components:\n",
    "  - **Precondition Code Accuracy**: Evaluates how well the generated code matches the precondition code.\n",
    "  - **New Lines Accuracy**: Assesses how well the generated code meets the goals by comparing the new lines added.\n",
    "- **Formula**: \n",
    "  \n",
    "  Weighted BLEU Score = (1 - α) * First BLEU Score + α * Second BLEU Score\n",
    "  \n",
    "  Where \\(\\alpha\\) is a weight factor, typically set to 0.5.\n",
    "- **Output**: A floating-point score indicating the degree of similarity between the generated and validation code.\n",
    "\n",
    "### 2. **Success Rate**\n",
    "- **Purpose**: Evaluates whether the generated code successfully performs the intended functionality.\n",
    "- **Description**: The generated code is executed in a testing environment, and the success rate is determined based on the test's result.\n",
    "- **Procedure**:\n",
    "  1. Modify the generated code to include screenshot commands.\n",
    "  2. Save the updated code to a temporary file.\n",
    "  3. Run the Playwright test.\n",
    "  4. Return `1` if the test passes, otherwise `0`.\n",
    "- **Output**: A binary score (`1` or `0`) representing test success.\n",
    "\n",
    "### 3. **Levenshtein Distance**\n",
    "- **Purpose**: Measures the similarity between the generated code and validation code based on edit distance.\n",
    "- **Description**: Calculates the number of single-character edits (insertions, deletions, or substitutions) needed to change the generated code into the validation code. The distance is normalized by the length of the longer code.\n",
    "- **Formula**:\n",
    "  Levenshtein Distance = Edit Distance / Max Length of Generated and Validation Code\n",
    "- **Output**: A floating-point score between 0 and 1, where lower values indicate higher similarity.\n",
    "\n",
    "### 4. **Similarity (Cosine Similarity)**\n",
    "- **Purpose**: Assesses the similarity between screenshots of the generated code and the ground truth.\n",
    "- **Description**: Uses a pre-trained ResNet-18 model to extract image embeddings and calculates the cosine similarity between embeddings of the predicted and ground truth images.\n",
    "- **Procedure**:\n",
    "  1. Load and preprocess the screenshots.\n",
    "  2. Compute embeddings using the ResNet-18 model.\n",
    "  3. Calculate cosine similarity between the embeddings.\n",
    "- **Output**: A floating-point score between 0 and 1, indicating the similarity between the images.\n",
    "\n",
    "## Evaluation Storage\n",
    "\n",
    "- **Evaluation Data**: Evaluation results for different templates and options can be found under `data/scores`.\n",
    "- **Evaluation Code**: The code used for evaluation is stored in `src/eval/metrics.py`.\n",
    "\n",
    "These metrics collectively provide a comprehensive assessment of the generated code's quality and effectiveness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Summary\n",
    "\n",
    "The evaluation of generated code was conducted across various templates and configurations, addressing different aspects such as image presence, HTML inclusion, and model fine-tuning. The following types of evaluations were performed:\n",
    "\n",
    "### 1. **Templates with Images**\n",
    "- **Examples**: `pred_test_script_pretr_T4_sc+_html+_single`, `pred_test_script_finetuned_T5_sc+_html+_single`\n",
    "- **Description**: Evaluated scripts that included image-based components, comparing generated outputs with expected results for scenarios where images were part of the test.\n",
    "\n",
    "### 2. **Templates without Images**\n",
    "- **Examples**: `pred_test_script_template_1_no_html_pretrained`, `pred_test_script_pretr_T1_sc-_html-_single`\n",
    "- **Description**: Focused on templates where no images were included. This evaluated the performance and accuracy of generated code in the absence of image-based validation.\n",
    "\n",
    "### 3. **HTML vs. No HTML**\n",
    "- **Examples**: `pred_test_script_pretr_T1_sc+_html-_single`, `pred_test_script_finetuned_T5_sc-_html+_single`\n",
    "- **Description**: Assessed how well the generated scripts handled scenarios with and without HTML components, testing their effectiveness in different contexts.\n",
    "\n",
    "### 4. **Single vs. All Configurations**\n",
    "- **Examples**: `pred_test_script_pretr_T5_sc+_html+_single`, `pred_test_script_pretr_T5_sc+_html+_all`\n",
    "- **Description**: Included evaluations for both single-instance and all-instance configurations to determine the performance across various levels of complexity and data variety.\n",
    "\n",
    "### 5. **Pretrained vs. Finetuned Models**\n",
    "- **Examples**: `pred_test_script_finetuned_T5_sc+_html+_single`, `pred_test_script_pretr_T1_sc-_html+_single`\n",
    "- **Description**: Compared results from pretrained models against those from finetuned models to assess improvements and differences in performance and accuracy.\n",
    "\n",
    "### 6. **Different Attribute Lengths and Concatenation Modes**\n",
    "- **Examples**: `pred_test_script_template_2_html_concat_mode_single_max_attr_length_50_pretrained`, `pred_test_script_template_2_html_concat_mode_all_max_attr_length_50_pretrained`\n",
    "- **Description**: Evaluated the impact of different attribute lengths and concatenation modes on the performance of generated code.\n",
    "\n",
    "\n",
    "This evaluation approach ensured a comprehensive assessment of the generated code under multiple conditions and configurations, providing insights into the effectiveness and accuracy of the different methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "##Evaluation\n",
    "In this section, we evaluate the generated code by comparing it with a validation code.\n",
    "We use several metrics such as weighted BLEU score, success rate, and Levenshtein distance.\n",
    "\"\"\"\n",
    "import os\n",
    "import esprima\n",
    "from Levenshtein import distance \n",
    "from src.evaluation.metrics import strip_script_code\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction  \n",
    "\n",
    "\n",
    "def calculate_scores(test_cases: List[dict], config: dict, metrics: list = None) -> dict:\n",
    "    \"\"\"Calculate scores for given metrics across multiple test cases.\n",
    "\n",
    "    :param test_cases: List of dictionaries, each containing 'generated_code', 'validation_code',\n",
    "                       'precondition_code', etc.\n",
    "    :param config: The configuration dictionary.\n",
    "    :param metrics: List of metrics to calculate the scores for, e.g., ['weighted bleu', 'success rate', 'levenshtein distance']\n",
    "    :return: Dictionary containing scores for each metric across all test cases.\n",
    "    \"\"\"\n",
    "    if metrics is None:\n",
    "        metrics = ['weighted bleu', 'success rate', 'levenshtein distance', \"similarity\"]\n",
    "    scores = {metric: [] for metric in metrics}\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        test = test_case.get('test_case', '')\n",
    "        test_step = test_case.get('test_step', '')\n",
    "        generated_code = test_case.get('generated_code', '')\n",
    "        validation_code = test_case.get('validation_code', '')\n",
    "        precondition_code = test_case.get('precondition_code', '')\n",
    "\n",
    "        image_folder_pred = os.path.normpath(config['paths']['eval_run_dir'])\n",
    "        image_folder_gt = os.path.normpath(config['paths']['gt_images'])\n",
    "        logger.debug(f\"Calculating scores for test case {test}_{test_step}...\")\n",
    "\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                match metric:\n",
    "                    case 'weighted bleu':\n",
    "                        scores[metric].append(\n",
    "                            calculate_weighted_bleu_score(generated_code, validation_code, precondition_code)\n",
    "                        )\n",
    "                    case 'success rate':\n",
    "                        file_name = test + \"_\" + test_step + \".spec.ts\"\n",
    "                        scores[metric].append(\n",
    "                            calculate_success_rate(generated_code, file_name=file_name, config=config)\n",
    "                        )\n",
    "                    case 'similarity':\n",
    "                        file_name = test + \"_\" + test_step\n",
    "                        screenshot_path_pred = os.path.join(image_folder_pred, f\"{file_name}.png\")\n",
    "                        screenshot_path_gt = os.path.join(image_folder_gt, f\"{file_name}.png\")\n",
    "\n",
    "                        model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "                        model.eval()\n",
    "\n",
    "                        if not os.path.exists(screenshot_path_gt):\n",
    "                            scores[metric].append(0)\n",
    "                        else:\n",
    "                            # Define your preprocessing steps here\n",
    "                            preprocess = transforms.Compose([\n",
    "                                transforms.Resize((224, 224)),  # Resize the images to the size expected by the model\n",
    "                                transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                                # Normalize the tensor\n",
    "                            ])\n",
    "                            scores[metric].append(\n",
    "                                encode_and_calculate_similarity(screenshot_path_pred, screenshot_path_gt, model=model,\n",
    "                                                                preprocess=preprocess)\n",
    "                            )\n",
    "                    case 'levenshtein distance':\n",
    "                        scores[metric].append(\n",
    "                            calculate_levenshtein_distance(generated_code, validation_code)\n",
    "                        )\n",
    "                    case _:\n",
    "                        scores[metric].append(None)\n",
    "                        logger.warning(f\"Unknown metric: {metric}. Skipping...\")\n",
    "            except Exception as e:\n",
    "                scores[metric].append(None)\n",
    "                logger.error(f\"Error calculating {metric} for test case {test}_{test_step}: {e}\")\n",
    "    return scores\n",
    "\n",
    "\n",
    "def encode_and_calculate_similarity(pred_img_path, gt_img_path, model, preprocess):\n",
    "    # Check if the prediction image path exists\n",
    "    if not os.path.exists(pred_img_path):\n",
    "        return 0\n",
    "\n",
    "    # Load and preprocess the images\n",
    "    image1 = Image.open(pred_img_path)\n",
    "    image2 = Image.open(gt_img_path)\n",
    "    input_tensor1 = preprocess(image1).unsqueeze(0)  # Create a mini-batch as expected by the model\n",
    "    input_tensor2 = preprocess(image2).unsqueeze(0)\n",
    "\n",
    "    # Encode the images\n",
    "    with torch.no_grad():\n",
    "        embedding1 = model(input_tensor1)\n",
    "        embedding2 = model(input_tensor2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cos_sim = cosine_similarity(embedding1.numpy(), embedding2.numpy())\n",
    "    return round(cos_sim.item(), 4)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Weighted BLEU Score  \n",
    "\n",
    "For the scoring of our generated predictions, we chose multiple scores. Our foremost score is the BLEU score, which is a frequent metric for LLMs, so we chose to implement it here as well.\n",
    "The BLEU score uses a similarity measure between the n-grams of the sample compared to the references he gets.<br>\n",
    "The reference in our case is the human-made code for the described test step. We chose to use the default configuration for the BLEU weights, which utilizes 1-grams up to 4-grams. All n-grams are uniformly weighted and have equal weight in the result."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Our weighted Bleu score separately evaluates the code from the precondition, which is copied by the LLM, and the newly generated code for the current step. This is intended to decouple the final result from the length of the precondition. Both parts are evaluated with 50 percent each."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def calculate_weighted_bleu_score(generated_code: str, validation_code: str, precondition_code: str,\n",
    "                                  alpha: float = 0.5) -> float:\n",
    "    \"\"\" This method returns the BLEU score of the given generated code.\n",
    "\n",
    "    :param generated_code: The generated code from the LLM as Python or TypeScript playwright script.\n",
    "    :param validation_code: Examples for validation as Python or TypeScript playwright script.\n",
    "    :param precondition_code: The precondition of the step as Python or TypeScript playwright script\n",
    "    :param alpha: The weight of the second part of the BLEU score.\n",
    "    :return: The BLEU score of the given generated code.\n",
    "    \"\"\"\n",
    "    generated_code_tokens = esprima.tokenize(generated_code)\n",
    "    validation_code_tokens = esprima.tokenize(validation_code)\n",
    "    precondition_code_tokens = esprima.tokenize(precondition_code)\n",
    "\n",
    "    # Convert tokens to string\n",
    "    generated_code_tokens = [str(elem) for elem in generated_code_tokens]\n",
    "    validation_code_tokens = [str(elem) for elem in validation_code_tokens]\n",
    "    precondition_code_tokens = [str(elem) for elem in precondition_code_tokens]\n",
    "\n",
    "    precondition_code_length = len(precondition_code_tokens)\n",
    "    precondition_code_length_without_end_lines = -1\n",
    "    for i in range(precondition_code_length):\n",
    "        if validation_code_tokens[i] != precondition_code_tokens[i]:\n",
    "            precondition_code_length_without_end_lines = i\n",
    "            break\n",
    "\n",
    "    # Define a smoothing function for BLEU score calculation\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "\n",
    "    # The first part: Has the LLM correctly copied the precondition code?\n",
    "    first_bleu_score = sentence_bleu(references=[validation_code_tokens[:precondition_code_length_without_end_lines]],\n",
    "                                     hypothesis=generated_code_tokens[:precondition_code_length_without_end_lines],\n",
    "                                     smoothing_function=smoothing_function)\n",
    "\n",
    "    # The second part: Has the LLM correctly added the new lines to reach the given goal?\n",
    "    second_bleu_score = sentence_bleu(references=[validation_code_tokens[precondition_code_length_without_end_lines:]],\n",
    "                                      hypothesis=generated_code_tokens[precondition_code_length_without_end_lines:],\n",
    "                                      smoothing_function=smoothing_function)\n",
    "\n",
    "    return (1 - alpha) * first_bleu_score + alpha * second_bleu_score\n",
    "\n",
    "\n",
    "def calculate_success_rate(generated_code: str, file_name: str, config: dict):\n",
    "    \"\"\"Returns the success rate of the given generated code.\"\"\"\n",
    "    try:\n",
    "        # Normalize paths\n",
    "        eval_run_dir = os.path.normpath(config['paths']['eval_run_dir'])\n",
    "        screen_shot_dir = os.path.join(eval_run_dir, 'screenshots')\n",
    "\n",
    "        # Logging paths and current working directory\n",
    "        logger.debug(f\"Current working directory: {os.getcwd()}\")\n",
    "        logger.debug(f\"Screenshot directory: {screen_shot_dir}\")\n",
    "        logger.debug(f\"Evaluation run directory: {eval_run_dir}\")\n",
    "\n",
    "        # Create directories for screenshots\n",
    "        os.makedirs(screen_shot_dir, exist_ok=True)\n",
    "        file_name_png = file_name.split(\".\")[0]  # remove .spec.ts\n",
    "        screenshot_path = os.path.join(screen_shot_dir, f\"{file_name_png}.png\")\n",
    "        # Replace backslashes with forward slashes\n",
    "        screenshot_path = screenshot_path.replace(\"\\\\\", \"/\")\n",
    "        logger.debug(f\"Screenshot path: {screenshot_path}\")\n",
    "\n",
    "        screenshot_code = f\"  await page.screenshot({{ path: '{screenshot_path}' }});\\n\"\n",
    "        time_out = 30000\n",
    "        time_out_code = f\"  test.setTimeout({time_out});\\n\"\n",
    "\n",
    "        generated_code = generated_code.split(\"\\n\")\n",
    "\n",
    "        #### insert screenshot code\n",
    "        # Find the position to insert the screenshot command\n",
    "        assert type(generated_code) == type([])\n",
    "        insert_position = 0\n",
    "        for i, line in enumerate(generated_code):\n",
    "            if 'async' in line and 'test(' in line:\n",
    "                insert_position = i + 1\n",
    "                break\n",
    "\n",
    "        # Insert the timeout code and screenshot command\n",
    "        generated_code.insert(insert_position, time_out_code)\n",
    "        found_position = 0\n",
    "        last_await_position = 0\n",
    "        for i, line in enumerate(generated_code):\n",
    "            if \"await page.close()\" in line or \"await context.close()\" in line or \"await browser.close()\" in line:\n",
    "                found_position = 1\n",
    "                insert_position = i\n",
    "                break\n",
    "            if \"await\" in line:\n",
    "                last_await_position = i + 1  # position after the last \"await\" line\n",
    "\n",
    "        if found_position == 0:\n",
    "            insert_position = last_await_position\n",
    "\n",
    "        # Insert the screenshot and HTML extraction commands\n",
    "        generated_code.insert(insert_position, screenshot_code)\n",
    "        ####\n",
    "\n",
    "        # Ensure the directory for the temp_path exists\n",
    "        temp_dir = os.path.join(eval_run_dir, \"test_script\")\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        logger.debug(f\"Created temp directory: {temp_dir}\")\n",
    "\n",
    "        temp_path = os.path.join(temp_dir, file_name)\n",
    "        logger.debug(f\"Temp file path: {temp_path}\")\n",
    "\n",
    "        # Save updated test code to a temporary file\n",
    "        try:\n",
    "            with open(temp_path, 'w', encoding=\"utf-8\") as file:\n",
    "                file.write(\"\\n\".join(generated_code))\n",
    "            logger.debug(f\"File {temp_path} created successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create the file {temp_path}. Error: {e}\")\n",
    "            return 0\n",
    "\n",
    "        # Small delay to account for file system delays\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Run the Playwright test\n",
    "        try:\n",
    "            logger.debug(f\"Current working directory: {os.getcwd()}\")\n",
    "            result = os.system(f\"npx playwright test {temp_path} --config=config/playwright.config.ts\")\n",
    "            score = 1 if result == 1 else 0\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to run Playwright test. Error: {e}\")\n",
    "            return 0\n",
    "\n",
    "        # Delete temp file after test run if defined in config\n",
    "        if config.get('evaluation', {}).get('delete_temp_files', False):\n",
    "            try:\n",
    "                os.remove(temp_path)\n",
    "                logger.debug(f\"Deleted temp file {temp_path}.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to delete temp file {temp_path}. Error: {e}\")\n",
    "\n",
    "        logger.debug(f\"Playwright test result: {result}\")\n",
    "        return score\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "        return 0\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Levenshtein Distance\n",
    "The scoring function for our validation samples is the Levenshtein distance. This measures the distance between two strings by the amount of necessary single-character operations to turn one string into another. These operations are remove, add, and replace. So the maximum distance this measure can calculate is the maximum length of input strings. Since later tests involve longer preconditions and descriptions the chance for mistakes is higher, so the Levenshtein distance would always increase for the longer test and be lower for the shorter tests. To counteract this, we normalize the distance to values between 0 and 1 by dividing with the maximum length of the input strings. This represents small errors or deviations in tests much better since those should perform better than very short and error-riddled tests."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_levenshtein_distance(generated_code, validation_code):\n",
    "    \"\"\" This method returns the Levenshtein distance of the given generated code.\n",
    "\n",
    "    :param generated_code: The generated code from the LLM as TypeScript playwright script.\n",
    "    :param validation_code: Examples for validation TypeScript playwright script.\n",
    "    :return: The Levenshtein distance over the length of the max code length of the given generated code.\n",
    "    \"\"\"\n",
    "    gen_script = ' '.join(strip_script_code(generated_code))\n",
    "    vd_script = ' '.join(strip_script_code(validation_code))\n",
    "\n",
    "    len_gen = len(generated_code)\n",
    "    len_valid = len(validation_code)\n",
    "    max_len = max(len_gen, len_valid)\n",
    "\n",
    "    score = distance(gen_script, vd_script) / max_len\n",
    "\n",
    "    return score\n",
    "\n",
    "# Validate the generated code (if validation_path is provided)\n",
    "if validation_path:\n",
    "    validation_code = parse_code(validation_path)\n",
    "    scores = calculate_scores(generated_code=generated_code, validation_code=validation_code, precondition_code=precondition_text, programming_language='Python')  # Adjust language as needed\n",
    "    print(\"Validation Scores:\", scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
